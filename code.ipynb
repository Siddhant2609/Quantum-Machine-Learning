{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulfSIDozdfkZ"
      },
      "source": [
        "# Quantum Machine Learning\n",
        "\n",
        "In this tutorial we will learn how to implement various Quantum Machine Learning Models and will try to benchmark their progress and accuracy in doing a simple classification task.\n",
        "\n",
        "\n",
        "--------------\n",
        "\n",
        "\n",
        "This tutorial will contain some mathematical background on the QML models and we will test the QML model on a randomly generated data set and a dataset from High Energy Physics (CERN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyhCm3jbfZIC",
        "outputId": "0294a9a5-d29c-4c6c-a551-d811f967cbde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting qiskit\n",
            "  Using cached qiskit-0.39.2-py3-none-any.whl\n",
            "Collecting qiskit-aer==0.11.1\n",
            "  Using cached qiskit_aer-0.11.1-cp37-cp37m-win_amd64.whl (24.3 MB)\n",
            "Collecting qiskit-ibmq-provider==0.19.2\n",
            "  Using cached qiskit_ibmq_provider-0.19.2-py3-none-any.whl (240 kB)\n",
            "Requirement already satisfied: qiskit-terra==0.22.2 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.16.3 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-aer==0.11.1->qiskit) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-aer==0.11.1->qiskit) (1.7.3)\n",
            "Requirement already satisfied: websocket-client>=1.0.1 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (1.4.2)\n",
            "Requirement already satisfied: requests>=2.19 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (2.25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (2.8.1)\n",
            "Requirement already satisfied: websockets>=10.0 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (10.4)\n",
            "Requirement already satisfied: requests-ntlm>=1.1.0 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (1.1.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (1.26.2)\n",
            "Requirement already satisfied: shared-memory38 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-terra==0.22.2->qiskit) (0.1.2)\n",
            "Requirement already satisfied: psutil>=5 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-terra==0.22.2->qiskit) (5.8.0)\n",
            "Requirement already satisfied: importlib-metadata<5.0 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-terra==0.22.2->qiskit) (3.1.1)\n",
            "Requirement already satisfied: tweedledum<2.0,>=1.1 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-terra==0.22.2->qiskit) (1.1.1)\n",
            "Requirement already satisfied: ply>=3.10 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-terra==0.22.2->qiskit) (3.11)\n",
            "Requirement already satisfied: dill>=0.3 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-terra==0.22.2->qiskit) (0.3.6)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-terra==0.22.2->qiskit) (4.1.1)\n",
            "Requirement already satisfied: retworkx>=0.11.0 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-terra==0.22.2->qiskit) (0.12.0)\n",
            "Requirement already satisfied: sympy>=1.3 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-terra==0.22.2->qiskit) (1.7.1)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from qiskit-terra==0.22.2->qiskit) (3.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata<5.0->qiskit-terra==0.22.2->qiskit) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.8.0->qiskit-ibmq-provider==0.19.2->qiskit) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit) (2020.6.20)\n",
            "Requirement already satisfied: cryptography>=1.3 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit) (38.0.3)\n",
            "Requirement already satisfied: ntlm-auth>=1.0.2 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit) (1.5.0)\n",
            "Requirement already satisfied: rustworkx==0.12.0 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from retworkx>=0.11.0->qiskit-terra==0.22.2->qiskit) (0.12.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from stevedore>=3.0.0->qiskit-terra==0.22.2->qiskit) (5.9.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sympy>=1.3->qiskit-terra==0.22.2->qiskit) (1.2.1)\n",
            "Requirement already satisfied: cffi>=1.12 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit) (1.15.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit) (2.21)\n",
            "Installing collected packages: qiskit-ibmq-provider, qiskit-aer, qiskit\n",
            "Successfully installed qiskit-0.39.2 qiskit-aer-0.11.1 qiskit-ibmq-provider-0.19.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.26.0-py3-none-any.whl (1.0 MB)\n",
            "     ---------------------------------------- 1.0/1.0 MB 1.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: cachetools in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pennylane) (4.2.0)\n",
            "Requirement already satisfied: retworkx in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pennylane) (0.12.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pennylane) (1.21.6)\n",
            "Collecting autograd\n",
            "  Downloading autograd-1.5-py3-none-any.whl (48 kB)\n",
            "     ---------------------------------------- 48.9/48.9 kB 2.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: toml in c:\\users\\monit sharma\\appdata\\roaming\\python\\python37\\site-packages (from pennylane) (0.10.1)\n",
            "Collecting semantic-version>=2.7\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting autoray>=0.3.1\n",
            "  Downloading autoray-0.5.1-py3-none-any.whl (37 kB)\n",
            "Collecting pennylane-lightning>=0.26\n",
            "  Downloading PennyLane_Lightning-0.26.1-cp37-cp37m-win_amd64.whl (4.3 MB)\n",
            "     ---------------------------------------- 4.3/4.3 MB 2.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: scipy in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pennylane) (1.7.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pennylane) (2.6.3)\n",
            "Collecting appdirs\n",
            "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-win_amd64.whl (313 kB)\n",
            "     -------------------------------------- 313.0/313.0 kB 1.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: future>=0.15.2 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from autograd->pennylane) (0.18.2)\n",
            "Requirement already satisfied: rustworkx==0.12.0 in c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from retworkx->pennylane) (0.12.0)\n",
            "Installing collected packages: ninja, appdirs, semantic-version, autoray, autograd, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autograd-1.5 autoray-0.5.1 ninja-1.11.1 pennylane-0.26.0 pennylane-lightning-0.26.1 semantic-version-2.10.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\monit sharma\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install qiskit\n",
        "!pip install pennylane"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZANWXd7weEe2"
      },
      "source": [
        "## Downloading the Data and the Preprocessing\n",
        "\n",
        "The dataset can be found [here](https://figshare.com/articles/dataset/Training_and_testing_data_used_in_the_paper_An_equation-of-state-meter_of_QCD_transition_from_deep_learning_/5457220/1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CWLJ7Uh3dOey"
      },
      "outputs": [],
      "source": [
        "# do the basic imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xaqNzTeff1QH"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('training_observables.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "h8AQPfDIf8mD",
        "outputId": "46507a5a-6301-47f8-e2e2-488e90dffd03"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493280</td>\n",
              "      <td>119.32780</td>\n",
              "      <td>0.026617</td>\n",
              "      <td>0.019539</td>\n",
              "      <td>0.001285</td>\n",
              "      <td>0.004125</td>\n",
              "      <td>-0.114632</td>\n",
              "      <td>-0.192545</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001793</td>\n",
              "      <td>0.003997</td>\n",
              "      <td>0.008152</td>\n",
              "      <td>0.015164</td>\n",
              "      <td>0.025790</td>\n",
              "      <td>0.040464</td>\n",
              "      <td>0.059261</td>\n",
              "      <td>0.081969</td>\n",
              "      <td>0.108310</td>\n",
              "      <td>0.138916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.499796</td>\n",
              "      <td>140.66150</td>\n",
              "      <td>0.027218</td>\n",
              "      <td>0.015303</td>\n",
              "      <td>0.017385</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>1.389377</td>\n",
              "      <td>0.282950</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>0.001750</td>\n",
              "      <td>0.003415</td>\n",
              "      <td>0.006048</td>\n",
              "      <td>0.009945</td>\n",
              "      <td>0.015720</td>\n",
              "      <td>0.024593</td>\n",
              "      <td>0.038424</td>\n",
              "      <td>0.059534</td>\n",
              "      <td>0.090801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.503392</td>\n",
              "      <td>152.76870</td>\n",
              "      <td>0.011552</td>\n",
              "      <td>0.016412</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>0.007357</td>\n",
              "      <td>-0.133998</td>\n",
              "      <td>0.223442</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003183</td>\n",
              "      <td>0.006916</td>\n",
              "      <td>0.013742</td>\n",
              "      <td>0.025196</td>\n",
              "      <td>0.042973</td>\n",
              "      <td>0.068877</td>\n",
              "      <td>0.104854</td>\n",
              "      <td>0.153033</td>\n",
              "      <td>0.215902</td>\n",
              "      <td>0.298440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.488244</td>\n",
              "      <td>123.84110</td>\n",
              "      <td>0.016648</td>\n",
              "      <td>0.007232</td>\n",
              "      <td>0.004221</td>\n",
              "      <td>0.001482</td>\n",
              "      <td>1.066655</td>\n",
              "      <td>0.425053</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.001691</td>\n",
              "      <td>0.003347</td>\n",
              "      <td>0.005789</td>\n",
              "      <td>0.008923</td>\n",
              "      <td>0.013181</td>\n",
              "      <td>0.020897</td>\n",
              "      <td>0.036646</td>\n",
              "      <td>0.066004</td>\n",
              "      <td>0.117576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.496821</td>\n",
              "      <td>130.78380</td>\n",
              "      <td>0.029870</td>\n",
              "      <td>0.034460</td>\n",
              "      <td>0.016352</td>\n",
              "      <td>0.001592</td>\n",
              "      <td>0.849569</td>\n",
              "      <td>-0.411059</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000919</td>\n",
              "      <td>0.002004</td>\n",
              "      <td>0.003871</td>\n",
              "      <td>0.006358</td>\n",
              "      <td>0.008433</td>\n",
              "      <td>0.007623</td>\n",
              "      <td>0.003397</td>\n",
              "      <td>0.026416</td>\n",
              "      <td>0.078568</td>\n",
              "      <td>0.177591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22570</th>\n",
              "      <td>22570</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.495708</td>\n",
              "      <td>43.28272</td>\n",
              "      <td>0.031320</td>\n",
              "      <td>0.010866</td>\n",
              "      <td>0.007185</td>\n",
              "      <td>0.000848</td>\n",
              "      <td>-0.414526</td>\n",
              "      <td>-0.690625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000392</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>0.001672</td>\n",
              "      <td>0.003088</td>\n",
              "      <td>0.005341</td>\n",
              "      <td>0.008532</td>\n",
              "      <td>0.012373</td>\n",
              "      <td>0.016067</td>\n",
              "      <td>0.018169</td>\n",
              "      <td>0.015897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22571</th>\n",
              "      <td>22571</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.536097</td>\n",
              "      <td>35.77644</td>\n",
              "      <td>0.089789</td>\n",
              "      <td>0.015850</td>\n",
              "      <td>0.006881</td>\n",
              "      <td>0.005601</td>\n",
              "      <td>-0.028845</td>\n",
              "      <td>0.934089</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001865</td>\n",
              "      <td>0.003920</td>\n",
              "      <td>0.007764</td>\n",
              "      <td>0.015250</td>\n",
              "      <td>0.029065</td>\n",
              "      <td>0.051837</td>\n",
              "      <td>0.085413</td>\n",
              "      <td>0.130882</td>\n",
              "      <td>0.189289</td>\n",
              "      <td>0.264222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22572</th>\n",
              "      <td>22572</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.516989</td>\n",
              "      <td>40.66633</td>\n",
              "      <td>0.056547</td>\n",
              "      <td>0.031709</td>\n",
              "      <td>0.012869</td>\n",
              "      <td>0.007707</td>\n",
              "      <td>0.561080</td>\n",
              "      <td>0.025548</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003741</td>\n",
              "      <td>0.007919</td>\n",
              "      <td>0.014907</td>\n",
              "      <td>0.025046</td>\n",
              "      <td>0.037916</td>\n",
              "      <td>0.052361</td>\n",
              "      <td>0.066622</td>\n",
              "      <td>0.078427</td>\n",
              "      <td>0.085027</td>\n",
              "      <td>0.082829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22573</th>\n",
              "      <td>22573</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.514398</td>\n",
              "      <td>48.36263</td>\n",
              "      <td>0.059926</td>\n",
              "      <td>0.013307</td>\n",
              "      <td>0.006346</td>\n",
              "      <td>0.004572</td>\n",
              "      <td>0.499208</td>\n",
              "      <td>-1.016149</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002169</td>\n",
              "      <td>0.004623</td>\n",
              "      <td>0.008857</td>\n",
              "      <td>0.015174</td>\n",
              "      <td>0.023454</td>\n",
              "      <td>0.033371</td>\n",
              "      <td>0.044792</td>\n",
              "      <td>0.058167</td>\n",
              "      <td>0.075026</td>\n",
              "      <td>0.098845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22574</th>\n",
              "      <td>22574</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.523754</td>\n",
              "      <td>58.44409</td>\n",
              "      <td>0.051211</td>\n",
              "      <td>0.039774</td>\n",
              "      <td>0.009579</td>\n",
              "      <td>0.002640</td>\n",
              "      <td>-0.014750</td>\n",
              "      <td>0.692910</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000898</td>\n",
              "      <td>0.001922</td>\n",
              "      <td>0.003896</td>\n",
              "      <td>0.007616</td>\n",
              "      <td>0.014228</td>\n",
              "      <td>0.025525</td>\n",
              "      <td>0.044245</td>\n",
              "      <td>0.073989</td>\n",
              "      <td>0.119131</td>\n",
              "      <td>0.186828</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22575 rows × 87 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0    1         2          3         4         5         6   \\\n",
              "0          0  0.0  0.493280  119.32780  0.026617  0.019539  0.001285   \n",
              "1          1  0.0  0.499796  140.66150  0.027218  0.015303  0.017385   \n",
              "2          2  0.0  0.503392  152.76870  0.011552  0.016412  0.004306   \n",
              "3          3  0.0  0.488244  123.84110  0.016648  0.007232  0.004221   \n",
              "4          4  0.0  0.496821  130.78380  0.029870  0.034460  0.016352   \n",
              "...      ...  ...       ...        ...       ...       ...       ...   \n",
              "22570  22570  1.0  0.495708   43.28272  0.031320  0.010866  0.007185   \n",
              "22571  22571  1.0  0.536097   35.77644  0.089789  0.015850  0.006881   \n",
              "22572  22572  1.0  0.516989   40.66633  0.056547  0.031709  0.012869   \n",
              "22573  22573  1.0  0.514398   48.36263  0.059926  0.013307  0.006346   \n",
              "22574  22574  1.0  0.523754   58.44409  0.051211  0.039774  0.009579   \n",
              "\n",
              "             7         8         9   ...        77        78        79  \\\n",
              "0      0.004125 -0.114632 -0.192545  ...  0.001793  0.003997  0.008152   \n",
              "1      0.001732  1.389377  0.282950  ...  0.000811  0.001750  0.003415   \n",
              "2      0.007357 -0.133998  0.223442  ...  0.003183  0.006916  0.013742   \n",
              "3      0.001482  1.066655  0.425053  ...  0.000763  0.001691  0.003347   \n",
              "4      0.001592  0.849569 -0.411059  ...  0.000919  0.002004  0.003871   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "22570  0.000848 -0.414526 -0.690625  ...  0.000392  0.000845  0.001672   \n",
              "22571  0.005601 -0.028845  0.934089  ...  0.001865  0.003920  0.007764   \n",
              "22572  0.007707  0.561080  0.025548  ...  0.003741  0.007919  0.014907   \n",
              "22573  0.004572  0.499208 -1.016149  ...  0.002169  0.004623  0.008857   \n",
              "22574  0.002640 -0.014750  0.692910  ...  0.000898  0.001922  0.003896   \n",
              "\n",
              "             80        81        82        83        84        85        86  \n",
              "0      0.015164  0.025790  0.040464  0.059261  0.081969  0.108310  0.138916  \n",
              "1      0.006048  0.009945  0.015720  0.024593  0.038424  0.059534  0.090801  \n",
              "2      0.025196  0.042973  0.068877  0.104854  0.153033  0.215902  0.298440  \n",
              "3      0.005789  0.008923  0.013181  0.020897  0.036646  0.066004  0.117576  \n",
              "4      0.006358  0.008433  0.007623  0.003397  0.026416  0.078568  0.177591  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "22570  0.003088  0.005341  0.008532  0.012373  0.016067  0.018169  0.015897  \n",
              "22571  0.015250  0.029065  0.051837  0.085413  0.130882  0.189289  0.264222  \n",
              "22572  0.025046  0.037916  0.052361  0.066622  0.078427  0.085027  0.082829  \n",
              "22573  0.015174  0.023454  0.033371  0.044792  0.058167  0.075026  0.098845  \n",
              "22574  0.007616  0.014228  0.025525  0.044245  0.073989  0.119131  0.186828  \n",
              "\n",
              "[22575 rows x 87 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's see the dataset\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsLwD52pgEOG"
      },
      "source": [
        "Let us name every column of the dataset their desired value,(what they actually represent in the high energy physics realm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SbveT2rLgN35"
      },
      "outputs": [],
      "source": [
        "# Every Feature name\n",
        "header_list = {\n",
        "    0 : 'number',\n",
        "    1 : 'eos_type',\n",
        "    2 :\t'mean_pt',\n",
        "    3 :\t'dndy',\n",
        "    4 :\t'v2',\n",
        "    5 :\t'v3',\n",
        "    6 :\t'v4',\n",
        "    7 : 'v5',\n",
        "    8 :\t'psi2',\n",
        "    9 :\t'psi3',\n",
        "    10 : 'psi4',\n",
        "    11 :\t'psi5',\n",
        "    12 :'ptspec_bin0',\n",
        "    13 : 'ptspec_bin',\n",
        "    14 : 'ptspec_bin2',\n",
        "    15 :\t'ptspec_bin3',\n",
        "    16 : 'ptspec_bin4',\n",
        "    17 :\t'ptspec_bin5',\n",
        "    18 :\t'ptspec_bin6',\n",
        "    19 :\t'ptspec_bin7',\n",
        "    20 :\t'ptspec_bin8',\n",
        "    21 : \t'ptspec_bin9',\n",
        "    22 :\t'ptspec_bin10',\n",
        "    23 :\t'ptspec_bin11',\n",
        "    24 :\t'ptspec_bin12',\n",
        "    25 :\t'ptspec_bin13',\n",
        "    26 :\t'ptspec_bin14',\n",
        "    27 :\t'v2_ptbin0',\n",
        "    28 :\t'v2_ptbin1',\n",
        "    29 :\t'v2_ptbin2',\n",
        "    30 :\t'v2_ptbin3',\n",
        "    31 :\t'v2_ptbin4',\n",
        "    32 :\t'v2_ptbin5',\n",
        "    33 :\t'v2_ptbin6',\n",
        "    34 :\t'v2_ptbin7',\n",
        "    35 :\t'v2_ptbin8',\n",
        "    36 :\t'v2_ptbin9',\n",
        "    37 :\t'v2_ptbin10',\n",
        "    38 :\t'v2_ptbin11',\n",
        "    39 :\t'v2_ptbin12',\n",
        "    40 :\t'v2_ptbin13',\n",
        "    41 :\t'v2_ptbin14',\n",
        "    42 :\t'v3_ptbin0',\n",
        "    43 :\t'v3_ptbin1',\n",
        "    44 :\t'v3_ptbin2',\n",
        "    45 :\t'v3_ptbin3',\n",
        "    46 :\t'v3_ptbin4',\n",
        "    47 :\t'v3_ptbin5',\n",
        "    48 :\t'v3_ptbin6',\n",
        "    49 :\t'v3_ptbin7',\n",
        "    50 : 'v3_ptbin8',\n",
        "    51 : 'v3_ptbin9',\n",
        "    52 : 'v3_ptbin10',\n",
        "    53 : 'v3_ptbin11',\n",
        "    54 : 'v3_ptbin12',\n",
        "    55 : 'v3_ptbin13',\n",
        "    56 : 'v3_ptbin14',\n",
        "    57 : 'v4_ptbin0',\n",
        "    58 : 'v4_ptbin1',\n",
        "    59 : 'v4_ptbin2',\n",
        "    60 : 'v4_ptbin3',\n",
        "    61 : 'v4_ptbin4',\n",
        "    62 : 'v4_ptbin5',\n",
        "    63 : 'v4_ptbin6',\n",
        "    64 : 'v4_ptbin7',\n",
        "    65 : 'v4_ptbin8',\n",
        "    66 : 'v4_ptbin9',\n",
        "    67 : 'v4_ptbin10',\n",
        "    68 : 'v4_ptbin11',\n",
        "    69 : 'v4_ptbin12',\n",
        "    70 : 'v4_ptbin13',\n",
        "    71 : 'v4_ptbin14',\n",
        "    72 : 'v5_ptbin0',\n",
        "    73 : 'v5_ptbin1',\n",
        "    74 : 'v5_ptbin2',\n",
        "    75 : 'v5_ptbin3',\n",
        "    76 : 'v5_ptbin4',\n",
        "    77 : 'v5_ptbin5',\n",
        "    78 : 'v5_ptbin6',\n",
        "    79 : 'v5_ptbin7',\n",
        "    80 : 'v5_ptbin8',\n",
        "    81 : 'v5_ptbin9',\n",
        "    82 : 'v5_ptbin10',\n",
        "    83 : 'v5_ptbin11',\n",
        "    84 : 'v5_ptbin12',\n",
        "    85 : 'v5_ptbin13',\n",
        "    86 : 'v5_ptbin14'\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "4tF5jlLfgTCi",
        "outputId": "d507934e-2fa4-40c5-f451-c5f2580ef118"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>number</th>\n",
              "      <th>eos_type</th>\n",
              "      <th>mean_pt</th>\n",
              "      <th>dndy</th>\n",
              "      <th>v2</th>\n",
              "      <th>v3</th>\n",
              "      <th>v4</th>\n",
              "      <th>v5</th>\n",
              "      <th>psi2</th>\n",
              "      <th>psi3</th>\n",
              "      <th>...</th>\n",
              "      <th>v5_ptbin5</th>\n",
              "      <th>v5_ptbin6</th>\n",
              "      <th>v5_ptbin7</th>\n",
              "      <th>v5_ptbin8</th>\n",
              "      <th>v5_ptbin9</th>\n",
              "      <th>v5_ptbin10</th>\n",
              "      <th>v5_ptbin11</th>\n",
              "      <th>v5_ptbin12</th>\n",
              "      <th>v5_ptbin13</th>\n",
              "      <th>v5_ptbin14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493280</td>\n",
              "      <td>119.3278</td>\n",
              "      <td>0.026617</td>\n",
              "      <td>0.019539</td>\n",
              "      <td>0.001285</td>\n",
              "      <td>0.004125</td>\n",
              "      <td>-0.114632</td>\n",
              "      <td>-0.192545</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001793</td>\n",
              "      <td>0.003997</td>\n",
              "      <td>0.008152</td>\n",
              "      <td>0.015164</td>\n",
              "      <td>0.025790</td>\n",
              "      <td>0.040464</td>\n",
              "      <td>0.059261</td>\n",
              "      <td>0.081969</td>\n",
              "      <td>0.108310</td>\n",
              "      <td>0.138916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.499796</td>\n",
              "      <td>140.6615</td>\n",
              "      <td>0.027218</td>\n",
              "      <td>0.015303</td>\n",
              "      <td>0.017385</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>1.389377</td>\n",
              "      <td>0.282950</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>0.001750</td>\n",
              "      <td>0.003415</td>\n",
              "      <td>0.006048</td>\n",
              "      <td>0.009945</td>\n",
              "      <td>0.015720</td>\n",
              "      <td>0.024593</td>\n",
              "      <td>0.038424</td>\n",
              "      <td>0.059534</td>\n",
              "      <td>0.090801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.503392</td>\n",
              "      <td>152.7687</td>\n",
              "      <td>0.011552</td>\n",
              "      <td>0.016412</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>0.007357</td>\n",
              "      <td>-0.133998</td>\n",
              "      <td>0.223442</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003183</td>\n",
              "      <td>0.006916</td>\n",
              "      <td>0.013742</td>\n",
              "      <td>0.025196</td>\n",
              "      <td>0.042973</td>\n",
              "      <td>0.068877</td>\n",
              "      <td>0.104854</td>\n",
              "      <td>0.153033</td>\n",
              "      <td>0.215902</td>\n",
              "      <td>0.298440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.488244</td>\n",
              "      <td>123.8411</td>\n",
              "      <td>0.016648</td>\n",
              "      <td>0.007232</td>\n",
              "      <td>0.004221</td>\n",
              "      <td>0.001482</td>\n",
              "      <td>1.066655</td>\n",
              "      <td>0.425053</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.001691</td>\n",
              "      <td>0.003347</td>\n",
              "      <td>0.005789</td>\n",
              "      <td>0.008923</td>\n",
              "      <td>0.013181</td>\n",
              "      <td>0.020897</td>\n",
              "      <td>0.036646</td>\n",
              "      <td>0.066004</td>\n",
              "      <td>0.117576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.496821</td>\n",
              "      <td>130.7838</td>\n",
              "      <td>0.029870</td>\n",
              "      <td>0.034460</td>\n",
              "      <td>0.016352</td>\n",
              "      <td>0.001592</td>\n",
              "      <td>0.849569</td>\n",
              "      <td>-0.411059</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000919</td>\n",
              "      <td>0.002004</td>\n",
              "      <td>0.003871</td>\n",
              "      <td>0.006358</td>\n",
              "      <td>0.008433</td>\n",
              "      <td>0.007623</td>\n",
              "      <td>0.003397</td>\n",
              "      <td>0.026416</td>\n",
              "      <td>0.078568</td>\n",
              "      <td>0.177591</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 87 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   number  eos_type   mean_pt      dndy        v2        v3        v4  \\\n",
              "0       0       0.0  0.493280  119.3278  0.026617  0.019539  0.001285   \n",
              "1       1       0.0  0.499796  140.6615  0.027218  0.015303  0.017385   \n",
              "2       2       0.0  0.503392  152.7687  0.011552  0.016412  0.004306   \n",
              "3       3       0.0  0.488244  123.8411  0.016648  0.007232  0.004221   \n",
              "4       4       0.0  0.496821  130.7838  0.029870  0.034460  0.016352   \n",
              "\n",
              "         v5      psi2      psi3  ...  v5_ptbin5  v5_ptbin6  v5_ptbin7  \\\n",
              "0  0.004125 -0.114632 -0.192545  ...   0.001793   0.003997   0.008152   \n",
              "1  0.001732  1.389377  0.282950  ...   0.000811   0.001750   0.003415   \n",
              "2  0.007357 -0.133998  0.223442  ...   0.003183   0.006916   0.013742   \n",
              "3  0.001482  1.066655  0.425053  ...   0.000763   0.001691   0.003347   \n",
              "4  0.001592  0.849569 -0.411059  ...   0.000919   0.002004   0.003871   \n",
              "\n",
              "   v5_ptbin8  v5_ptbin9  v5_ptbin10  v5_ptbin11  v5_ptbin12  v5_ptbin13  \\\n",
              "0   0.015164   0.025790    0.040464    0.059261    0.081969    0.108310   \n",
              "1   0.006048   0.009945    0.015720    0.024593    0.038424    0.059534   \n",
              "2   0.025196   0.042973    0.068877    0.104854    0.153033    0.215902   \n",
              "3   0.005789   0.008923    0.013181    0.020897    0.036646    0.066004   \n",
              "4   0.006358   0.008433    0.007623    0.003397    0.026416    0.078568   \n",
              "\n",
              "   v5_ptbin14  \n",
              "0    0.138916  \n",
              "1    0.090801  \n",
              "2    0.298440  \n",
              "3    0.117576  \n",
              "4    0.177591  \n",
              "\n",
              "[5 rows x 87 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = dataset.rename(columns=header_list)\n",
        "dataset.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "HlDukPImgT6R",
        "outputId": "206b8545-0ab1-4fe3-d41d-ae207dbb9e84"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eos_type</th>\n",
              "      <th>mean_pt</th>\n",
              "      <th>dndy</th>\n",
              "      <th>v2</th>\n",
              "      <th>v3</th>\n",
              "      <th>v4</th>\n",
              "      <th>v5</th>\n",
              "      <th>psi2</th>\n",
              "      <th>psi3</th>\n",
              "      <th>psi4</th>\n",
              "      <th>...</th>\n",
              "      <th>v5_ptbin5</th>\n",
              "      <th>v5_ptbin6</th>\n",
              "      <th>v5_ptbin7</th>\n",
              "      <th>v5_ptbin8</th>\n",
              "      <th>v5_ptbin9</th>\n",
              "      <th>v5_ptbin10</th>\n",
              "      <th>v5_ptbin11</th>\n",
              "      <th>v5_ptbin12</th>\n",
              "      <th>v5_ptbin13</th>\n",
              "      <th>v5_ptbin14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493280</td>\n",
              "      <td>119.32780</td>\n",
              "      <td>0.026617</td>\n",
              "      <td>0.019539</td>\n",
              "      <td>0.001285</td>\n",
              "      <td>0.004125</td>\n",
              "      <td>-0.114632</td>\n",
              "      <td>-0.192545</td>\n",
              "      <td>0.604534</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001793</td>\n",
              "      <td>0.003997</td>\n",
              "      <td>0.008152</td>\n",
              "      <td>0.015164</td>\n",
              "      <td>0.025790</td>\n",
              "      <td>0.040464</td>\n",
              "      <td>0.059261</td>\n",
              "      <td>0.081969</td>\n",
              "      <td>0.108310</td>\n",
              "      <td>0.138916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.499796</td>\n",
              "      <td>140.66150</td>\n",
              "      <td>0.027218</td>\n",
              "      <td>0.015303</td>\n",
              "      <td>0.017385</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>1.389377</td>\n",
              "      <td>0.282950</td>\n",
              "      <td>0.155580</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>0.001750</td>\n",
              "      <td>0.003415</td>\n",
              "      <td>0.006048</td>\n",
              "      <td>0.009945</td>\n",
              "      <td>0.015720</td>\n",
              "      <td>0.024593</td>\n",
              "      <td>0.038424</td>\n",
              "      <td>0.059534</td>\n",
              "      <td>0.090801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.503392</td>\n",
              "      <td>152.76870</td>\n",
              "      <td>0.011552</td>\n",
              "      <td>0.016412</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>0.007357</td>\n",
              "      <td>-0.133998</td>\n",
              "      <td>0.223442</td>\n",
              "      <td>-0.593591</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003183</td>\n",
              "      <td>0.006916</td>\n",
              "      <td>0.013742</td>\n",
              "      <td>0.025196</td>\n",
              "      <td>0.042973</td>\n",
              "      <td>0.068877</td>\n",
              "      <td>0.104854</td>\n",
              "      <td>0.153033</td>\n",
              "      <td>0.215902</td>\n",
              "      <td>0.298440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.488244</td>\n",
              "      <td>123.84110</td>\n",
              "      <td>0.016648</td>\n",
              "      <td>0.007232</td>\n",
              "      <td>0.004221</td>\n",
              "      <td>0.001482</td>\n",
              "      <td>1.066655</td>\n",
              "      <td>0.425053</td>\n",
              "      <td>-0.017385</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.001691</td>\n",
              "      <td>0.003347</td>\n",
              "      <td>0.005789</td>\n",
              "      <td>0.008923</td>\n",
              "      <td>0.013181</td>\n",
              "      <td>0.020897</td>\n",
              "      <td>0.036646</td>\n",
              "      <td>0.066004</td>\n",
              "      <td>0.117576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.496821</td>\n",
              "      <td>130.78380</td>\n",
              "      <td>0.029870</td>\n",
              "      <td>0.034460</td>\n",
              "      <td>0.016352</td>\n",
              "      <td>0.001592</td>\n",
              "      <td>0.849569</td>\n",
              "      <td>-0.411059</td>\n",
              "      <td>0.742171</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000919</td>\n",
              "      <td>0.002004</td>\n",
              "      <td>0.003871</td>\n",
              "      <td>0.006358</td>\n",
              "      <td>0.008433</td>\n",
              "      <td>0.007623</td>\n",
              "      <td>0.003397</td>\n",
              "      <td>0.026416</td>\n",
              "      <td>0.078568</td>\n",
              "      <td>0.177591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22570</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.495708</td>\n",
              "      <td>43.28272</td>\n",
              "      <td>0.031320</td>\n",
              "      <td>0.010866</td>\n",
              "      <td>0.007185</td>\n",
              "      <td>0.000848</td>\n",
              "      <td>-0.414526</td>\n",
              "      <td>-0.690625</td>\n",
              "      <td>0.623115</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000392</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>0.001672</td>\n",
              "      <td>0.003088</td>\n",
              "      <td>0.005341</td>\n",
              "      <td>0.008532</td>\n",
              "      <td>0.012373</td>\n",
              "      <td>0.016067</td>\n",
              "      <td>0.018169</td>\n",
              "      <td>0.015897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22571</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.536097</td>\n",
              "      <td>35.77644</td>\n",
              "      <td>0.089789</td>\n",
              "      <td>0.015850</td>\n",
              "      <td>0.006881</td>\n",
              "      <td>0.005601</td>\n",
              "      <td>-0.028845</td>\n",
              "      <td>0.934089</td>\n",
              "      <td>0.126117</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001865</td>\n",
              "      <td>0.003920</td>\n",
              "      <td>0.007764</td>\n",
              "      <td>0.015250</td>\n",
              "      <td>0.029065</td>\n",
              "      <td>0.051837</td>\n",
              "      <td>0.085413</td>\n",
              "      <td>0.130882</td>\n",
              "      <td>0.189289</td>\n",
              "      <td>0.264222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22572</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.516989</td>\n",
              "      <td>40.66633</td>\n",
              "      <td>0.056547</td>\n",
              "      <td>0.031709</td>\n",
              "      <td>0.012869</td>\n",
              "      <td>0.007707</td>\n",
              "      <td>0.561080</td>\n",
              "      <td>0.025548</td>\n",
              "      <td>0.430158</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003741</td>\n",
              "      <td>0.007919</td>\n",
              "      <td>0.014907</td>\n",
              "      <td>0.025046</td>\n",
              "      <td>0.037916</td>\n",
              "      <td>0.052361</td>\n",
              "      <td>0.066622</td>\n",
              "      <td>0.078427</td>\n",
              "      <td>0.085027</td>\n",
              "      <td>0.082829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22573</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.514398</td>\n",
              "      <td>48.36263</td>\n",
              "      <td>0.059926</td>\n",
              "      <td>0.013307</td>\n",
              "      <td>0.006346</td>\n",
              "      <td>0.004572</td>\n",
              "      <td>0.499208</td>\n",
              "      <td>-1.016149</td>\n",
              "      <td>0.443735</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002169</td>\n",
              "      <td>0.004623</td>\n",
              "      <td>0.008857</td>\n",
              "      <td>0.015174</td>\n",
              "      <td>0.023454</td>\n",
              "      <td>0.033371</td>\n",
              "      <td>0.044792</td>\n",
              "      <td>0.058167</td>\n",
              "      <td>0.075026</td>\n",
              "      <td>0.098845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22574</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.523754</td>\n",
              "      <td>58.44409</td>\n",
              "      <td>0.051211</td>\n",
              "      <td>0.039774</td>\n",
              "      <td>0.009579</td>\n",
              "      <td>0.002640</td>\n",
              "      <td>-0.014750</td>\n",
              "      <td>0.692910</td>\n",
              "      <td>0.630324</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000898</td>\n",
              "      <td>0.001922</td>\n",
              "      <td>0.003896</td>\n",
              "      <td>0.007616</td>\n",
              "      <td>0.014228</td>\n",
              "      <td>0.025525</td>\n",
              "      <td>0.044245</td>\n",
              "      <td>0.073989</td>\n",
              "      <td>0.119131</td>\n",
              "      <td>0.186828</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22575 rows × 86 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       eos_type   mean_pt       dndy        v2        v3        v4        v5  \\\n",
              "0           0.0  0.493280  119.32780  0.026617  0.019539  0.001285  0.004125   \n",
              "1           0.0  0.499796  140.66150  0.027218  0.015303  0.017385  0.001732   \n",
              "2           0.0  0.503392  152.76870  0.011552  0.016412  0.004306  0.007357   \n",
              "3           0.0  0.488244  123.84110  0.016648  0.007232  0.004221  0.001482   \n",
              "4           0.0  0.496821  130.78380  0.029870  0.034460  0.016352  0.001592   \n",
              "...         ...       ...        ...       ...       ...       ...       ...   \n",
              "22570       1.0  0.495708   43.28272  0.031320  0.010866  0.007185  0.000848   \n",
              "22571       1.0  0.536097   35.77644  0.089789  0.015850  0.006881  0.005601   \n",
              "22572       1.0  0.516989   40.66633  0.056547  0.031709  0.012869  0.007707   \n",
              "22573       1.0  0.514398   48.36263  0.059926  0.013307  0.006346  0.004572   \n",
              "22574       1.0  0.523754   58.44409  0.051211  0.039774  0.009579  0.002640   \n",
              "\n",
              "           psi2      psi3      psi4  ...  v5_ptbin5  v5_ptbin6  v5_ptbin7  \\\n",
              "0     -0.114632 -0.192545  0.604534  ...   0.001793   0.003997   0.008152   \n",
              "1      1.389377  0.282950  0.155580  ...   0.000811   0.001750   0.003415   \n",
              "2     -0.133998  0.223442 -0.593591  ...   0.003183   0.006916   0.013742   \n",
              "3      1.066655  0.425053 -0.017385  ...   0.000763   0.001691   0.003347   \n",
              "4      0.849569 -0.411059  0.742171  ...   0.000919   0.002004   0.003871   \n",
              "...         ...       ...       ...  ...        ...        ...        ...   \n",
              "22570 -0.414526 -0.690625  0.623115  ...   0.000392   0.000845   0.001672   \n",
              "22571 -0.028845  0.934089  0.126117  ...   0.001865   0.003920   0.007764   \n",
              "22572  0.561080  0.025548  0.430158  ...   0.003741   0.007919   0.014907   \n",
              "22573  0.499208 -1.016149  0.443735  ...   0.002169   0.004623   0.008857   \n",
              "22574 -0.014750  0.692910  0.630324  ...   0.000898   0.001922   0.003896   \n",
              "\n",
              "       v5_ptbin8  v5_ptbin9  v5_ptbin10  v5_ptbin11  v5_ptbin12  v5_ptbin13  \\\n",
              "0       0.015164   0.025790    0.040464    0.059261    0.081969    0.108310   \n",
              "1       0.006048   0.009945    0.015720    0.024593    0.038424    0.059534   \n",
              "2       0.025196   0.042973    0.068877    0.104854    0.153033    0.215902   \n",
              "3       0.005789   0.008923    0.013181    0.020897    0.036646    0.066004   \n",
              "4       0.006358   0.008433    0.007623    0.003397    0.026416    0.078568   \n",
              "...          ...        ...         ...         ...         ...         ...   \n",
              "22570   0.003088   0.005341    0.008532    0.012373    0.016067    0.018169   \n",
              "22571   0.015250   0.029065    0.051837    0.085413    0.130882    0.189289   \n",
              "22572   0.025046   0.037916    0.052361    0.066622    0.078427    0.085027   \n",
              "22573   0.015174   0.023454    0.033371    0.044792    0.058167    0.075026   \n",
              "22574   0.007616   0.014228    0.025525    0.044245    0.073989    0.119131   \n",
              "\n",
              "       v5_ptbin14  \n",
              "0        0.138916  \n",
              "1        0.090801  \n",
              "2        0.298440  \n",
              "3        0.117576  \n",
              "4        0.177591  \n",
              "...           ...  \n",
              "22570    0.015897  \n",
              "22571    0.264222  \n",
              "22572    0.082829  \n",
              "22573    0.098845  \n",
              "22574    0.186828  \n",
              "\n",
              "[22575 rows x 86 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = dataset.drop(['number'], axis = 1)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K29CB3CgbT6"
      },
      "source": [
        "Since the number of qubits required scales with the number of features , we'll try to keep it less "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pKQPKGppgX_K"
      },
      "outputs": [],
      "source": [
        "important_features = [\n",
        "    \n",
        "    'eos_type',\n",
        "\t'v2',\n",
        "\t'v3',\n",
        " 'psi2'\n",
        "\t   \n",
        "]\n",
        "\n",
        "# Best features according to reference\n",
        "\n",
        "\n",
        "# The index of the best features in the header_list\n",
        "important_features_id = np.array([\n",
        "                         0,\n",
        "                         4,\n",
        "                         5,\n",
        "                         8\n",
        "                         \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNUQhvD1gkYq",
        "outputId": "5d014b6f-1f09-4860-bb30-66daaa09725f"
      },
      "outputs": [],
      "source": [
        "# Convert it to numpy\n",
        "dataset = dataset.to_numpy()\n",
        "\n",
        "# Sepparating the label (Y) from the input features (X)\n",
        "Y = dataset[:, 0]\n",
        "Y = np.array(Y, dtype=int)\n",
        "X = dataset[:, 1:]\n",
        "\n",
        "# Sanity check\n",
        "print(X.shape, Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzjRMKn8gnet",
        "outputId": "8ab7d23e-38d2-4f4f-901d-f415b426616e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13434, 85) (13434,)\n",
            "(9141, 85) (9141,)\n"
          ]
        }
      ],
      "source": [
        "# Separate data with label 0 and label 1\n",
        "\n",
        "x_0 = X[Y == 0, :]\n",
        "x_1 = X[Y == 1, :]\n",
        "y_0 = Y[Y==0]\n",
        "y_1 = Y[Y==1]\n",
        "\n",
        "# Sanity check\n",
        "print(x_0.shape, y_0.shape)\n",
        "print(x_1.shape, y_1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qJl5aDmVhzJl"
      },
      "outputs": [],
      "source": [
        "# Split 50% of the data for training set and 50% for testing set\n",
        "\n",
        "x_train_0, x_test_0, y_train_0, y_test_0 = train_test_split(x_0, y_0, test_size=0.5, random_state=2021)\n",
        "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(x_1, y_1, test_size=0.5, random_state=2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBA26Sjsh1rw",
        "outputId": "27a3cb33-ff55-4bad-bbd3-be68a0f86ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4000, 85) (4000,)\n",
            "(4000, 85) (4000,)\n"
          ]
        }
      ],
      "source": [
        "num_sample = 2000 # sample per class, total = 400\n",
        "\n",
        "# Take the first 400 samples (500 from each class) from training set for X_train and Y_train\n",
        "X_train = np.concatenate((x_train_0[:num_sample, :], x_train_1[:num_sample, :]), axis=0)\n",
        "Y_train = np.concatenate((y_train_0[:num_sample], y_train_1[:num_sample]), axis=0)\n",
        "\n",
        "# Take the first 1000 samples (500 from each class) from testing set for X_test and Y_test\n",
        "X_test = np.concatenate((x_test_0[:num_sample, :], x_test_1[:num_sample, :]), axis=0)\n",
        "Y_test = np.concatenate((y_test_0[:num_sample], y_test_1[:num_sample]), axis=0)\n",
        "\n",
        "# Sanity Check\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Fw-WHvgJh4aY"
      },
      "outputs": [],
      "source": [
        "# Save the 400 subset as a txt file for future use so we don't need to load the whole dataset each time\n",
        "\n",
        "np.savetxt('./QCD Dataset/X_train_4000.txt', X_train)\n",
        "np.savetxt('./QCD Dataset/X_test_4000.txt', X_test)\n",
        "np.savetxt('./QCD Dataset/Y_train_4000.txt', Y_train)\n",
        "np.savetxt('./QCD Dataset/Y_test_4000.txt', Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrhVfCTth63a",
        "outputId": "3971f7e6-bf45-49c4-cc64-e2515060bcd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4000, 85) (4000,)\n",
            "(4000, 85) (4000,)\n"
          ]
        }
      ],
      "source": [
        "# Code to load the saved subset\n",
        "\n",
        "num_sample = 4000\n",
        "\n",
        "X_train = np.loadtxt('./QCD Dataset/X_train_' + str(num_sample) + '.txt')\n",
        "Y_train = np.loadtxt('./QCD Dataset/Y_train_' + str(num_sample) + '.txt')\n",
        "\n",
        "X_test = np.loadtxt('./QCD Dataset/X_test_' + str(num_sample) + '.txt')\n",
        "Y_test = np.loadtxt('./QCD Dataset/Y_test_' + str(num_sample) + '.txt')\n",
        "\n",
        "# Sanity Check\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siVgAMmXh8uX",
        "outputId": "a6ac2424-8476-4b94-f26f-48dd103761a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((4000, 3), (4000, 3))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Take only the best 3 features\n",
        "\n",
        "X_train = X_train[:, important_features_id[:3]]\n",
        "X_test = X_test[:, important_features_id[:3]]\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBpH3hQ-ilI6",
        "outputId": "18265cd5-b642-4061-bc76-9d320880e23c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.5727371 , 0.01961521, 0.01268726],\n",
              "       [0.5031572 , 0.01139411, 0.00381489],\n",
              "       [0.493949  , 0.00356587, 0.00591594],\n",
              "       ...,\n",
              "       [0.4378398 , 0.01194924, 0.00095084],\n",
              "       [0.4434155 , 0.00266166, 0.00173735],\n",
              "       [0.4957438 , 0.01178781, 0.00435343]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZJ0BsewinOd",
        "outputId": "2a446646-2b3d-42de-fc2e-6c608d2decd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.5949019 , 0.01120026, 0.0141949 ],\n",
              "       [0.566241  , 0.0397017 , 0.0133887 ],\n",
              "       [0.4653798 , 0.01649949, 0.00153883],\n",
              "       ...,\n",
              "       [0.5077894 , 0.00496598, 0.00181929],\n",
              "       [0.5087496 , 0.00478042, 0.00345181],\n",
              "       [0.5005321 , 0.00357258, 0.00427089]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O9hUuRDirku",
        "outputId": "411ce2ff-9bb1-45dd-8829-cec140fccfea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 1., 1., 1.])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_uCyypLiutQ",
        "outputId": "85995154-9160-41c6-f534-5682fb0a0c22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 1., 1., 1.])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB6aMnu4ixdS"
      },
      "source": [
        "Here you can see the Y data only includes string of $0$ and $1$s because its a case of binary classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1iFBlG2ncna"
      },
      "source": [
        "## Quantum Support Vector Machine \n",
        "\n",
        "This implementation is based on [Schuld and Killoran(2018)](https://arxiv.org/abs/1803.07128)\n",
        "\n",
        "We can basically replace the commonly used \"variational approach\" to quantum machine learning with a classical kernel method where the kernel is computed using a quantum device.\n",
        "\n",
        "We will see how a kernel based method compares to that of a variational method.\n",
        "We will combine pennylane with scikit-learn and then compare this whole with the variational quantum circuit trained via stochastic gradient descent using PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hECi3q8CoLRZ"
      },
      "source": [
        "It's our hypotheses that **for quantum machine learning applications with many parameters, kernel-based training can be a great alternative to the variational approach to quantum machine learning**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mWna_tHojLM"
      },
      "source": [
        "## Some Mathematical Formulation\n",
        "\n",
        "Let's consider a quantum model of the form :\n",
        "\n",
        "$$ f(x) = \\langle \\phi(x) | \\mathcal{M} | \\phi(x)\\rangle, $$\n",
        "\n",
        "where $| \\phi(x)\\rangle$ is prepared by a fixed embedding circuit that encodes data input $x$ and $\\mathcal{M}$ is an arbitrary observable. \n",
        "\n",
        "\n",
        "### Kernel Approach\n",
        "\n",
        "Instead of trainig the $f$ variationally, we can often train an equivalent classical kernel method with a kernel executed on a quantum device. \n",
        "The quantum kernel is given by the mutual overlap of two data-encoding quantum states,\n",
        "\n",
        "$$ \\kappa(x, x') = | \\langle \\phi(x') | \\phi(x)\\rangle|^2. $$\n",
        "\n",
        "hence its only based on data-encoding.\n",
        "\n",
        "If the Loss function $L$ is the [hinge loss](https://en.wikipedia.org/wiki/Hinge_loss), the kernel method corresponds to a standard [support vector machine](https://en.wikipedia.org/wiki/Support-vector_machine) in the sense of a maximum-margin classifier.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHGtzYiHp7qq"
      },
      "source": [
        "### Note:\n",
        "\n",
        "Specifically , we can replace variational method for qml with the kernel based training, if the optimization problem can be written as minimizing a cost of the form:\n",
        "\n",
        "$$ \\min_f  \\lambda\\;  \\mathrm{tr}\\{\\mathcal{M}^2\\} + \\frac{1}{M}\\sum_{m=1}^M L(f(x^m), y^m), $$\n",
        "\n",
        "which is a regularized empirical risk with training data samples $(x^m, y^m)_{m=1\\dots M}$ regularization strength $\\lambda \\in \\mathbb{R}$ and the loss function $L$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RnHmdMnqds7"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "We will do some basic but useful imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-l__U2-1iwT3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.functional import relu\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane.templates import AngleEmbedding, StronglyEntanglingLayers\n",
        "from pennylane.operation import Tensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POVOIrwLqpgd"
      },
      "source": [
        "The technique we are going to use to embedd the data is the **angle-embedding**, which needs as many qubits as there are number of features. \n",
        "\n",
        "(There is a way to run for arbitrary number of features by just using one qubit, you can checkk that [here](https://twitter.com/MonitSharma1729/status/1577953151134552066?s=20&t=sWbk8aem3eM_Z2Z6YRQVQg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maXZCNedjVOb",
        "outputId": "2da96dd0-ac89-4f19-b293-b0d042849696"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_qubits = len(X_train[0])\n",
        "n_qubits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0MHC-zJriXv"
      },
      "source": [
        "To implement the kernel, we need to prepare two states : $| \\phi(x) \\rangle$,$| \\phi(x') \\rangle$ on different sets of qubits with the help of angle-embedding routines $S(x), S(x^{\\prime})$ and measure their overlap with a small routine called as [SWAP test](https://en.wikipedia.org/wiki/Swap_test)\n",
        "\n",
        "\n",
        "-------------------------\n",
        "\n",
        "What we can try further on is just take half the number of qubits to prepare $| \\phi(x) \\rangle$ and then apply the inverse embedding with $x^{\\prime}$ on the same qubits. And finally measure the projector onto initial states.\n",
        "\n",
        "\n",
        "\n",
        "------------------------------------\n",
        "\n",
        "\n",
        "Let us verify that this gives us the kernel:\n",
        "\n",
        "$$ \\begin{split}\\begin{align*}\n",
        "    \\langle 0..0 |S(x') S(x)^{\\dagger} \\mathcal{M} S(x')^{\\dagger} S(x)  | 0..0\\rangle &= \\langle 0..0 |S(x') S(x)^{\\dagger} |0..0\\rangle \\langle 0..0| S(x')^{\\dagger} S(x)  | 0..0\\rangle  \\\\\n",
        "    &= |\\langle 0..0| S(x')^{\\dagger} S(x)  | 0..0\\rangle |^2\\\\\n",
        "    &= | \\langle \\phi(x') | \\phi(x)\\rangle|^2 \\\\\n",
        "    &= \\kappa(x, x').\n",
        "\\end{align*}\\end{split} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2xDwoCKshuZ"
      },
      "source": [
        "All this thing applied together, we have our *quantum kernel evaluator*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XY9X6GTcjZJs"
      },
      "outputs": [],
      "source": [
        "dev_kernel = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "projector = np.zeros((2**n_qubits, 2**n_qubits))\n",
        "projector[0, 0] = 1\n",
        "\n",
        "@qml.qnode(dev_kernel)\n",
        "def kernel(x1, x2):\n",
        "    \"\"\"The quantum kernel.\"\"\"\n",
        "    AngleEmbedding(x1, wires=range(n_qubits))\n",
        "    qml.adjoint(AngleEmbedding)(x2, wires=range(n_qubits))\n",
        "    return qml.expval(qml.Hermitian(projector, wires=range(n_qubits)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V0UpYXpsrZg"
      },
      "source": [
        "Check whether evaluating the kernel of a data point and itself returns 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekP93O9-jczC",
        "outputId": "a14aae28-c279-4fe0-dbca-0569cabeaec5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1., requires_grad=True)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kernel(X_train[0], X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o281wzUdsx67"
      },
      "source": [
        "The way an SVM with a custom kernel is implemented in scikit-learn requires us to pass a function that computes a matrix of kernel evaluations for samples in two different datasets A, B. If A=B, this is the [Gram matrix](https://en.wikipedia.org/wiki/Gramian_matrix)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "uNoTsAoajfkZ"
      },
      "outputs": [],
      "source": [
        "def kernel_matrix(A, B):\n",
        "    \"\"\"Compute the matrix whose entries are the kernel\n",
        "       evaluated on pairwise data from sets A and B.\"\"\"\n",
        "    return np.array([[kernel(a, b) for b in B] for a in A])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfon00i8s8It"
      },
      "source": [
        "Training the SVM optimizes internal parameters that basically weigh kernel functions. It is a breeze in scikit-learn, which is designed as a high-level machine learning library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AhLuQzkCjhiM"
      },
      "outputs": [],
      "source": [
        "svm = SVC(kernel=kernel_matrix).fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fx6KmjXs_od"
      },
      "source": [
        "Let’s compute the accuracy on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoVD0fcCjmUo",
        "outputId": "70bdab91-c4fc-468a-eefe-a212ed862a47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8275"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = svm.predict(X_test)\n",
        "accuracy_score(predictions, Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoI5-mfxtDoM"
      },
      "source": [
        "How many times was the quantum device evaluated?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuysVh-ujoWs",
        "outputId": "e5a94689-cece-48aa-8fed-3aa8da3c725e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "320001"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_kernel.num_executions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyxclgvttGNq"
      },
      "source": [
        "## How to get this number?\n",
        "\n",
        "For $M$ training samples, the SVM must construct the $M \\times M$ dimensional kernel gram -matrix for training. To classify $M_{pred}$ new samples, the SVM needs to evaluate the kernel at most $M_{pred}M$ times to get the pairwise distances between training vectors and test samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRlYLrLYtj5F"
      },
      "source": [
        "Let us formulate this as a function, which can be used at the end for the plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb5F278Ijsm6"
      },
      "outputs": [],
      "source": [
        "def circuit_evals_kernel(n_data, split):\n",
        "    \"\"\"Compute how many circuit evaluations one needs for kernel-based\n",
        "       training and prediction.\"\"\"\n",
        "\n",
        "    M = int(np.ceil(split * n_data))\n",
        "    Mpred = n_data - M\n",
        "\n",
        "    n_training = M * M\n",
        "    n_prediction = M * Mpred\n",
        "\n",
        "    return n_training + n_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ3-KTMsjvRA",
        "outputId": "d2dc6a26-325c-4c14-b05e-ca929a287c01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "254826600"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "circuit_evals_kernel(n_data=len(X), split=len(X_train) /(len(X_train) + len(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdJykidqtrlj"
      },
      "source": [
        "## Using the variational Approach\n",
        "\n",
        "Using the variational principle of training, we can propose an ansatz for the variational circuit and train it directly. By increasing the number of layers of the ansatz, its expressivity increases. Depending on the ansatz, we may only search through a subspace of all measurements for the best candidate.\n",
        "\n",
        "Remember from above, the variational training does not optimize exactly the same cost as the SVM, but we try to match them as closely as possible. For this we use a bias term in the quantum model, and train on the hinge loss.\n",
        "\n",
        "We also explicitly use the parameter-shift differentiation method in the quantum node, since this is a method which works on hardware as well. While `diff_method='backprop'` or `diff_method='adjoint'` would reduce the number of circuit evaluations significantly, they are based on tricks that are only suitable for simulators, and can therefore not scale to more than a few dozen qubits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvdRh3hHjxNv"
      },
      "outputs": [],
      "source": [
        "dev_var = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev_var, interface=\"torch\", diff_method=\"parameter-shift\")\n",
        "def quantum_model(x, params):\n",
        "    \"\"\"A variational quantum model.\"\"\"\n",
        "\n",
        "    # embedding\n",
        "    AngleEmbedding(x, wires=range(n_qubits))\n",
        "\n",
        "    # trainable measurement\n",
        "    StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "def quantum_model_plus_bias(x, params, bias):\n",
        "    \"\"\"Adding a bias.\"\"\"\n",
        "    return quantum_model(x, params) + bias\n",
        "\n",
        "def hinge_loss(predictions, targets):\n",
        "    \"\"\"Implements the hinge loss.\"\"\"\n",
        "    all_ones = torch.ones_like(targets)\n",
        "    hinge_loss = all_ones - predictions * targets\n",
        "    # trick: since the max(0,x) function is not differentiable,\n",
        "    # use the mathematically equivalent relu instead\n",
        "    hinge_loss = relu(hinge_loss)\n",
        "    return hinge_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEbPFw13t56b"
      },
      "source": [
        "We now summarize the usual training and prediction steps into two functions similar to scikit-learn’s `fit()` and `predict()`. While it feels cumbersome compared to the one-liner used to train the kernel method, PennyLane—like other differentiable programming libraries—provides a lot more control over the particulars of training.\n",
        "\n",
        "In our case, most of the work is to convert between numpy and torch, which we need for the differentiable `relu` function used in the hinge loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzRPlqXzj16w"
      },
      "outputs": [],
      "source": [
        "def quantum_model_train(n_layers, steps, batch_size):\n",
        "    \"\"\"Train the quantum model defined above.\"\"\"\n",
        "\n",
        "    params = np.random.random((n_layers, n_qubits, 3))\n",
        "    params_torch = torch.tensor(params, requires_grad=True)\n",
        "    bias_torch = torch.tensor(0.0)\n",
        "\n",
        "    opt = torch.optim.Adam([params_torch, bias_torch], lr=0.1)\n",
        "\n",
        "    loss_history = []\n",
        "    for i in range(steps):\n",
        "\n",
        "        batch_ids = np.random.choice(len(X_train), batch_size)\n",
        "\n",
        "        X_batch = X_train[batch_ids]\n",
        "        y_batch = Y_train[batch_ids]\n",
        "\n",
        "        X_batch_torch = torch.tensor(X_batch, requires_grad=False)\n",
        "        y_batch_torch = torch.tensor(y_batch, requires_grad=False)\n",
        "\n",
        "        def closure():\n",
        "            opt.zero_grad()\n",
        "            preds = torch.stack(\n",
        "                [quantum_model_plus_bias(x, params_torch, bias_torch) for x in X_batch_torch]\n",
        "            )\n",
        "            loss = torch.mean(hinge_loss(preds, y_batch_torch))\n",
        "\n",
        "            # bookkeeping\n",
        "            current_loss = loss.detach().numpy().item()\n",
        "            loss_history.append(current_loss)\n",
        "            if i % 10 == 0:\n",
        "                print(\"step\", i, \", loss\", current_loss)\n",
        "\n",
        "            loss.backward()\n",
        "            return loss\n",
        "\n",
        "        opt.step(closure)\n",
        "\n",
        "    return params_torch, bias_torch, loss_history\n",
        "\n",
        "\n",
        "def quantum_model_predict(X_pred, trained_params, trained_bias):\n",
        "    \"\"\"Predict using the quantum model defined above.\"\"\"\n",
        "\n",
        "    p = []\n",
        "    for x in X_pred:\n",
        "\n",
        "        x_torch = torch.tensor(x)\n",
        "        pred_torch = quantum_model_plus_bias(x_torch, trained_params, trained_bias)\n",
        "        pred = pred_torch.detach().numpy().item()\n",
        "        if pred > 0:\n",
        "            pred = 1\n",
        "        else:\n",
        "            pred = -1\n",
        "\n",
        "        p.append(pred)\n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YavvN5cauCCz"
      },
      "source": [
        "Train the variational model and see how well we are doing on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "6TYtevCKj9um",
        "outputId": "80673046-7b74-43f6-f1c3-03c699f87978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 , loss 0.8563191001013569\n",
            "step 10 , loss 0.5615959659240116\n",
            "step 20 , loss 0.45986020671957056\n",
            "step 30 , loss 0.5095650636919851\n",
            "step 40 , loss 0.6034720145867367\n",
            "step 50 , loss 0.5017926799968434\n",
            "step 60 , loss 0.2521063628336219\n",
            "step 70 , loss 0.7007138524371598\n",
            "step 80 , loss 0.3506985253461143\n",
            "step 90 , loss 0.3508393315769453\n",
            "accuracy on test set: 0.8\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO29eZxjZ3nn+3u0Ha21SareXV3uzW5sHEzbBgzYASfYQOy5ySTBmewkzswdMjDJzQzcEJI4M3cmYQIhDJPESViTCwOEEEMcuGC8sHhr23h3t9u9uPdS7drX9/5xznv0nqNzpKMqqUpVer6fT3+6pFJJr87yPvvzkBACDMMwzPDiW+8FMAzDMOsLCwKGYZghhwUBwzDMkMOCgGEYZshhQcAwDDPksCBgGIYZcvomCIjok0Q0Q0TPuvyeiOjPiegYET1NRFf3ay0MwzCMO/20CD4N4OY2v78FwD7j3x0A/qKPa2EYhmFc6JsgEEI8CGC+zUtuA/BZofMwgDEi2tav9TAMwzDOBNbxs3cAOK08PmM8d97+QiK6A7rVgFgs9trLLrtsTRbIMAyzWXj88cdnhRBpp9+tpyDwjBDiLgB3AcChQ4fE4cOH13lFDMMwGwsiOuX2u/XMGjoLYJfyeKfxHMMwDLOGrKcguBvALxrZQ68DsCSEaHELMQzDMP2lb64hIvo8gBsBpIjoDIDfBxAEACHEXwK4B8DbARwDUADwK/1aC8MwDONO3wSBEOL2Dr8XAP59vz6fYRiG8QZXFjMMwww5LAgYhmGGHBYEDMMwQw4LAoZhmCGHBQHDMMyQw4KAYRhmyGFBwDAMM+SwIGAYhhlyWBAwDMMMOSwIGIZhhhwWBAzDMEMOCwKGYZghhwUBwzDMkMOCgGEYZshhQcAwDDPksCBgGIYZclgQMAzDDDksCBiGYYYcFgQMwzBDDgsChmGYIYcFAcMwzJDDgoBhGGbIYUHAMAwz5LAgYBiGGXKGRhAUKjW8eGF5vZfBMAwzcAyNIPjk907g5j/7LgqV2novhWEYZqAYGkGwOxUDAJyaK6zzShiGYQaL4REESV0QnJzNr/NKGIZhBovhEQSGRXBijgUBwzCMytAIgrgWQCqu4dQsu4YYhmFUhkYQAMB0KsoWAcMwjI2hEgRTyRjHCBiGYWwMlSCYTsUwky0jX+YUUoZhGMlQCQIzc4jdQwzDMCbDJQhSUQBcS8AwDKPSV0FARDcT0REiOkZE73f4/SVEdB8RPUlETxPR2/u5ninDIjjBcQKGYRiTvgkCIvID+ASAWwAcBHA7ER20veyDAL4ohHgNgHcB+F/9Wg+gp5CmExoHjBmGYRT6aRFcC+CYEOK4EKIC4AsAbrO9RgAYMX4eBXCuj+sBAEwnYxwjYAaeE7N5vP1j38VcrrzeS2GGgH4Kgh0ATiuPzxjPqfwBgJ8nojMA7gHwm05vRER3ENFhIjqcyWRWtajdqShOcoyAGXAeenkOz59fxssZVlqY/rPeweLbAXxaCLETwNsBfI6IWtYkhLhLCHFICHEonU6v6gOnkjFksmXkOIWUGWCk1ZrnbrnMGtBPQXAWwC7l8U7jOZV3A/giAAghHgIQBpDq45owneLmc8zgI6/PQrm+zithhoF+CoLHAOwjomkiCkEPBt9te80rAN4KAER0OXRBsDrfTwe4loDZCLBFwKwlfRMEQogagPcA+CaAF6BnBz1HRHcS0a3Gy34bwK8T0VMAPg/gl4UQol9rApq1BGwRMINKoyHMWpcCuzCZNSDQzzcXQtwDPQisPvch5efnAVzfzzXYiYYCmExoHDBmBpYLyyWUaw0AQL7CriGm/6x3sHhd2J3i5nPM4KK6LbkvFrMWDKUg2Iy1BM+eXcKFpdJ6L4PpASeVmRkFtgiYNaCvrqFBZXcqhtlcBYuFCsaiofVezqrJZMv4if/5PQgBXJqK4fV7kvjNt+zD1tHwei+NWQGn5vIIBXwYjwbZImDWhKG0CN64V89Q/asHj6/zSnrDQqECIYB3vnobppJR/P0jr+Afn7Rn6jK1egM/ODa73svoyInZPKYmoohrgaGzCCq1Bqr1xnovY+gYSkFw5c5R/ORrduBvv3tiU8QK5Gbxk1fvwCd/+Rr4fcSapAPfeXEGP/c3j+B4JrfeS2nLqbkCppIxxLXA0KWP/sbnDuND//Tsei9j3Tg1l8eVf/DNNb9Gh1IQAMB/vuUyBPyE//LPL6z3UlZNwdgsIsEAiAjRkJ8rpx1YLunHZKFQXeeVuNNoCJycy2M6FUU0FBi6grLTC8Wh7g58fDaPbKm25sdgaAXBlpEwfvMt+/DtFy7iwaN9rWHrO0XDIoiG/ACAWChgCgemSbmmH6dBPjYXs3rq6FQyhpjmHzqLoFyrI1saru+sIgX/WqcND60gAIBffeNu7E5G8Ydfe25D+yULNkEQ1fycf+5AuWrk5g+wli01welUTLcIhuw8lquNobZmpeBf60LCoRYEWsCP33vnQbycyeMv7395TT7zI986il/85KM9fU/TItD0JLBYKMAVqQ7IIq1BtghkRfFUMqpbBEN2Hsu1xpBbBPp3Z4tgjXnr5Vvwzldvw59/5yUcuZDt++c9fWYRj5+c7+l7Si0iGjRcQ2wROCJdQ4N8bE7O6qmj20cjw2kR1OrIDbEgkNcmWwTrwB/e+iqMhIP4nS8/hVqfXUTz+QrylTqypd4FLOVmEVFiBMOmSXrBtAgG+NicnMvjkokofD5CLKTHCPrcfmtgEEKgXGugUm+gVB0uASiR1ipbBOtAMq7hD297FZ4+s4S//u6Jvn7WXK4CAD2tAi5W6vARoAX00xkdwvxzL8jNZbAtgoLZITeqBSAEUBySTbFaF5Ayb1jjBDJ+tdbuSxYEBu+4chve9qot+Oi3j+L0fP8a0s3l9dGDF5Z7JwgKlTqiIT11FICuSQ7pjdSOQbcI1NRRQD+PwGAHt3uJdN0BGNo4gWkRrPE5Z0FgQET4wC2Xo1Jr4L4jM335jEKlhpKRuXK+lxZBtWa6hQCsm2+50RADXblrZg0NqEWgpo4C+nkEBju43UukoAYwtHECeW2utSLHgkBhKhnF1pEwHj3R22CuRLqFAOBiDwWBbhE0BYHMP19r3/KDL2Xwc3/zCJ4/t7ymn+uVQa8jUFNHAf08AoNvEQgh8OzZpVW/jyoIsuXuY2iNhsBSsffFgvP5yopjFqVqHTNZ7/d6M2uIBcG6QUS4ZnoCj52c78smOpdvCoLzPXYNRYKqINB9y9L6WCukoLvYxYW/lpg9/gd0Yz0zXwQAXDJhuIa0jWERPHVmCe/8+Pfww9OLq3qfcnV1rqGvPX0O1//37/Rcm/6pv/gBPv6dl1b0t3c9eBxv/9j3PO8nzRgBu4bWlWt3j+PichlnFoo9f+95Iz7go95aBEW7RWD8vNYBN5kJtTSgLRwGvY4gk9Ovj3RCA9B0DQ2qK0syZ6x7Nlte1fus1jV0ZqGIXLmG80u9u3eFEHhlvoCzK9wPTszmMZsre74X82aMgC2CdeWa6QkA6It7SGrMe9LxnsYICpWauWkA6+dbzpq9fCodXrk+lAc8ayiTLWMkHEBYqQcBBje4LSmax3V167S4hlaQXi3dNzOrFEgquXIN9YZYcfA6Y6wl43FN0hJgi2Cd2T+ZwEg4gMd6XPQFNF1DB7eP4GKvXUO2GAGw9i6QrLFhLQ66RTCgG2smWzatAUCvBwEGV3BJerV5qX74lVizssLe66brBXktr5UgkJYAp4+uMz4f4dDuCTzaB0Ewn69AC/iwJx3HXL5iSZdbDYVK3XQHAetpERiuoT4E7HpB0zU0mBurXRBId9+gurIkZn3GKgWs1SLo/r0K1d4LAnktZ1f43aS7T/7fiYKZNcQWwbpzze4JHM/kTd9nr5jLVZCMhczJYTPLvXl/3SJouoZkkHGtNcnlQXcN1XrjwugXmVwZ6URzqpx5Hgc0uC0p9sgisASLV7DxlqRF0MP7tmkRdK/cVOsNzBteAC/CSQiBfKUGIt3dVm+sXdYfCwIHrp0eBwA8dnKhp+87ny9jIh7C1hH9Zu9VUVmxUmtJHwXWPuAkA3wD6xqqStfQYG6smWwZ6XjTItACPvho8C0CU4vtaYxgBa6hPlgEUqlZiatqVhFIXtZUqjYgBJCM6eNz17KinAWBA1fsGIUW8PU8TjCXr2AippkWQS8CxkIIFKr2rCGpSa6Pa2hxwC2CSr2BSm2w2o4XKjXkyjWLa4iIjL5Rgym4JNI1tFoBKwXBaCSI3Ao08EI/YgTFZoyg25RydR1eAthSkKYMZWAtY1ksCBzQAn5ctWus94IgV0FKcQ31IoW0XNO1CGtlsfQtr3GwWFoEgxojqDbg07twDJyWPZvVhacqCAB9tsSgrdVO77KG9PdJxUMDYxEsGUpNvSG6rsuR6wj4yNOapCCV18Bapn+zIHDh2t0TeO7cck+16vl8BROxEBJaANGQvycWgTmUxlZQBqy9L9xMH80PqkXQwFhUN7sHLRMnk9OvBbsgiIUCA9+AzcwaWq1FYGy0ybg2cFlDQPdxArmOvZNxT2uS96u8BtZSkWNB4MI10xOoNwSefGV11ZKSQqWGYrWOiXgIRISto+GepJBKbVGtIzB9y2udPmrcKMul2ooCXX/8jRfxgMPY0PtenFl1D6NGQ6BSb2A8GgTQW7P7By/PrrrFgtwo1BgBIC2CwRJadnpdR5COa6uyCOYLlZ5NHFSt224D2PKcHtw24imAXbAJgrV07bIgcOHybQkAwPHZXE/eTxaTpWL6Sd46Eu5JBaTUglTXEBEhpq2tJllvCOQrdYwZG223KaTlWh1/9cDL+Maz51t+96ffOoKPf+fYqtZXMTaGpHH8e2kR/N9feQb/4/87sqr3kD7kFtfQBpgtUepV1pDhGkrGQyvK0pH3ghAws3VWi9Ui6FIQ5MoYjQSxYzyCuVy5o3KUk66hOFsEA0MqpiHk9624tNyOvDAnjIwA3SJYvQlrn1csWesB9lLo7BrX++R0GzA+PV9EQzRTUFWypdqqhZp0O4zHemsRVOsNnF4orjoVOJMtw0fN60MS3wCzJZq576u3CIJ+0oPF5e6Ds8VqHZOGIO2Ve2ipWEHIr2+T3ba9yGTLmExomExoaHgQTvKaNC2CNbx/WRC44PMRto+FcWaxx4IgbgiCEd011FhlrrC8WCI2QbDWA+ylBrdrIgKg+4DxqTm986bTzZYt1VY90U1qm3Kj7dWxObNQRL0hVt3WIJMtIxnX4JfRbIOoMaVskJEumdXXETSgBfyIawE0RPfvV6zUMZXUFZFeCYLFQhU7xvVreiUxgnRCMzf2TmuS16QZI1hD1y4LgjbsGI/0zCKQOcVJxSKoNQRm86u7YM3B9UqMAFj7AfbSbN41sTKLQLZgtt9sQghkS9XVWwSG/1kKgl5ZSycNATaf72z6t0Nqj3b08zjYFoGZPtqDrCEt4EMirFtt3ZxzIQSK1bp5/fVMEBSr2CkFQbcxgpxNEHSIE8jjN8kWwWCxYyyCsx0sgqVC1dOcY2kRJOPNGAEAXFzyfsEWK3V87NsvWVpTuLmGdE1yLS0Cu2uoW4tAnwpnv/nLtQaqdeHoMuoGeczGZdZQjzbXU4YAa4jm9LmVIDcNO1Ft8C0CeQ32QliHg37Ew7pS040GLlM7pyb0WQ69qC4WQmCpUMVO45ruJkYghMDMsl4gmI7r93pHi8CMEeiv5xjBgLBjLIpMtuw6lOLh43N4w3+/F39+b+de5fP5CkIBn9kTqFlU5t3iePClDD767aM4rFQ8F10Ege5bXkuLQL9pZS/9hS4FgdSs7TebfFypNVbVm0luFL23CJpjTVejhdqriiUxY9rcIA+wl66hUrWxKquoXGsYFoEUBN7PkVzDWDSIkXAAMz3IyCtW66jUG6ZF0E2MIF+po1itI53QkEro11yn66NgtJcYiQQQ8BFnDQ0K0jfoNGj+waMZ/PKnHkW+UsdxQytsx1xe7zMk5wqbRWVdXLByHaq27ZQ+CuiDz9eyIlXetNvHIiBqFuJ4RVoE9ptf1TJXM75QuoZ6bhHM5U2//krjBI2GwGwbi6DeEJb2C4NGqaJaqKs4R9U6QgEfElr3gkB+biToRzqh9cQikPdZMhZCNOTvykLJKFlg0VAAcS3gySKIGbPHo6G1TRtmQdCGHWO6ILC7h779/EX82mcOYzoVx+XbRjxljMzlykjGmxkhqZiGgI+6KiqTr1WbusmOi61ZQ2s7wF7eJCORAEYjwa4sgkqtgTMLBfh9hFy5Zgmgqzffagaal5XjpAV8PbMITs0VcHDbCICVWwRLxSqqdeEoCGJmJ9nBjBPIFidmfcYq1lmuNaAF/SuKEUirPRwyBEEPYgTyPhuLBpEId5eOnbGlA6cTWseRlYVKzewTFl/j9O++CgIiupmIjhDRMSJ6v8trfoaIniei54jo/+3nerpFmoRqwLhUreM9n38Cl21L4PO/fh32TcY9zSSdN/oMSXw+wmRC66rx3AXDjaTm6BcrdRDpRWQqvR5gX6k12qa/yUDaSDiI8Wioq6yhMwsFNASwbzIOAMgpm7S6+a/mxpAatRb0I6YFeuJ3r9UbOL1QwKHdepPClW4+9slkKlLAD2otQbUuUG8IM/a1mnXKYPFKYgTFin5+o0E/0olwTwSBnLQ3GgkhrgW6UkRaBEG8s3DKV+qm4I+usWu3b4KAiPwAPgHgFgAHAdxORAdtr9kH4AMArhdCvArA+/q1npWwdTQMIlhSSI9ezKJUbeDf3bAHY9EQJhMaLi6XO/pwpWvI/v5Obic3TItA2ZALlTqiQb/pcpL0eoD9p75/Ajd95AFXH3C2VEPQT9ACPoxGgm2zho5ezFoqP6Vb6ModowCsLiD15lv2uDHUG8Ih6KwLRS3g083uHriGzi2WUK0LXLY1gUS4s+nvhltVMaDOLR5Mi0D65lNxGXtZpUUQ8CG+GtdQyO9p0/WCVGZ0iyDYVdZQxlAO5Tn14q4qlGuIGhaBbtFvDtfQtQCOCSGOCyEqAL4A4Dbba34dwCeEEAsAIISY6eN6uibo92FLImyxCF44vwwAuNxwB2wZCaNYrXfUVuUsApWto+HuLIJl6RpSYwTWWQSSaKi3A+xPzRcwn6/gnEsWVbZURSIcBBFhPBp0zRpaLFTw9o99F5/5wUnzOZk6euXOUeO9VEHQfB+vMYIvHj6NN//JfRahZVoEAZ/e0XMF2tbfPXzKPP9AM8A9lYytyh1h1x5VTIvAYb1CCHzyeycclYmvPXUOL15Ybnm+18hkhZSLRfAPj5/xPMNarSMAVhYsjhiuoXyl7midNBoCXzx82lP3WXkNS9dQVzGCXBkBH5kxKS/XR14ZORtd44LQfgqCHQBOK4/PGM+p7Aewn4i+T0QPE9HNTm9ERHcQ0WEiOpzJtPai6Sc7xiM4u9jMDHnhfBbRkN/Mjpkc0W+AdlXCRSODYCJuFQTbRiM4t1j0VFQmhDAtgqViU9u2zyKQxLXeDrCXGr7c/OxkSzUz22MsGsJi0dkiOLtYRK0h8J0XmzL/1FwecS2AqWTMWLOy+ZedrYN2vGIILXXzlJXFWtC/ov49F5dL+OBXn8XHv9PMEJNFcNOp2Kq00HaCoDmcpvW7X1gu4c6vP4+vP32u5Xcf/Oqz+PA3Vtf2wgtNi6C1LcK5xSJ++0tP4WsO63OiXKtDC/rg9xFiIf+KYgSRoN/Mw5910MCfeGUB/+nLT+NBh55WduQ1PB4N6TGCLl1DqbgGn5FIkE7o/ZPcMhAB66TBmLZ5LAIvBADsA3AjgNsB/DURjdlfJIS4SwhxSAhxKJ1Or+kC7bUEL5xfxoGtCfMETxoTpdrFCWR+ud0iuDQdQ6nawHkPVsFCoWpqMapFkK/UHQVBr8dVSu3opEuGlFUQBLGYd9aeZGbN4ZML5tpOzhWwOxU1/37ZxTXkVSOThXSq+0d1DcVW0L9HbhzffWnWrBs5OVdAOOjT2wiMhFecqZLJlREONl0iKs0YQeumIAVI0UGoFSo1/ODluZ6NQ3VDfnbSrNhuHlcZU/Lad6pUbZixrkQ42JUGLgWQzBoCnLO4pMI27yGrbalQhRbw6bUNK4gRqILdS3VxrlxDVNt8FsFZALuUxzuN51TOALhbCFEVQpwAcBS6YBgYdoxHcH6xhHpDQAiBF84vm24hoGkRtMscavYZsmp8e9J6cPTlmc6N7WS9QSjgs2QNFW2D6yW9HmAvhc+J2YLj77OlqrmRjUVCyJZrjh0gM8ZxqtQbeOSEPu/h5FweU8kYRhzyx7OlKoJ+Xeh61RBlIZ3FIlBcQytJzZNdUbOlGp48rXekPTWXx+5kDES0aosgndBa4jwAzGPqtCmYgsCmZVbrehFesVrHYyd6O2XPTrFqDFNxaIsglQev502PERhZM11m6RSVrLB2m65U2Ly4qxYLVbOJYiIc7C5rKOciCNooC4WyzSLYJOmjjwHYR0TTRBQC8C4Ad9te81Xo1gCIKAXdVXS8j2vqmh1jEdQaAplsGeeWSlgu1XD51oT5+y0jHiyCnKwqtloEpiDIdBYE0g+8f0vcchEXXFxDvbYIZF3ACZdurLpFoN807TqQyuMUCvjw4NEMqvUGziwUsTsZRVwz0gZtmUJj0RBCAZ9njUx+Z1XrbwqC7rOG6g2B7740ix87uAV+H+GBI7pQODlXMHvbpBN6D/2VHG+3YjKgeR6dNgW50dmFmup+eOBof8NuMlvHySKQrhWvmr3MGgJg+OS7EAQVa/oo4CwI5HNurkuVxWIFYxH9e8l0Tq8Fc7KqWCJ/bqcstMQINkP6qBCiBuA9AL4J4AUAXxRCPEdEdxLRrcbLvglgjoieB3AfgN8RQsz1a00rQRaVnV0s4EVboBjQL5BoyN82RjAn20vYXEOpeAgj4YAnQSDjA5dtHcFisWpmAxUqdUSCrS4F0yLokVYhMyjUSloVu2sIcG4zkcmWkQgH8LpLk/juS7M4azRtm0rGlIrS5t8tG+87Eg54ztqQVpBqDZWrdfgICPqp66yhp84sYqlYxU9ctR1XXzKG+4/OoN4QeGWugN1GXMNrYzEn7G4EFXkenTYF+Vl2v7PqKrr/SH9jalITTzrECEyLwOOGrtcR6FtSt66YouIaGo+G4HeZCiaf81LnslioYtS0CLwPe6o3BObyFcs57dQVVQihxwiUrKFCtb7qppRe6WuMQAhxjxBivxBijxDivxrPfUgIcbfxsxBC/JYQ4qAQ4kohxBf6uZ6VsNMoKjuzUDQzRg4oFgGgWwXtKoTnjRiBvcUwEWHPZBwvz3SuTL6wVILfR9g3GUe90ey9U6w2Lx6VdkHGbinX6ihU6gj5fTg9X3DsrZQtVTFiWgT693RKIZ0xmqu9eV8Kx2ZyeOi4LvenUzFEQ374qDVAnAgb6XseNwb5nQs215AW0NNs411aBA8ezYAIeNPeFG48MIlnzy7jmbNLqNQbZoB7VYLApaoYAMIBP4hcLIKcs0UgN+eD20bw0kyuY7+s1SCP8WgkiKDf2hZhSZn32wkhBCqKa2ikyxhBsVpH0E8I+vVgczIWchYExjHz4hpaKlYxFrEKAi/fZaFQQb1hLRCciIVA5H59lGt6e46oUkcgBFDqc4xHst7B4oGnaREU8cL5LHZNREwXiESvGmxvEYQCzsHAPem4Z4tgS0IzNS95IRdcgsW9HGAvP+vg9hHUGgJnbB1ZhdDz9uXNMt7GItAFQRhv2qcH/T/30CkAwFQyam7SliKyUhUJTS/R9zrQvBkjaN5EpWrd1DajoQBK1YanZoGAHh+4aucYxmMh3LBfX/dnHzoJANidMrLHVigIqnW9UE82GrPj8xGiQX9bi8AeI5CC4ZYrturr76NVUFLSNu1FjFIR8GLJqTEcoPvK2kKljrAyrnVyxDlvvyvXkBIjcHJbuuGUBRbw+3Th5BIjkMfNjBGEepv11wkWBB2IhgIYjwZxdqGIFy4s4/KtIy2v2TISbtvkStYQOAUD96TjmMmWO2o/F5aL2DoaNjUUGTAuuriGejnAXrqFXnOJntB1wpZCmq/U0RBNrUn6VRccLYISJkc07N8Sx5YRDc+fX0bMKAIC0KL5S5dTNz5jqaUWynaLQL/cTXdLm1Q+yUK+gqdOL5oC4OC2EaTiGr7+lD5Jze4a6rbfkIwfuVkEgNE3qk2MoMU1ZDy+YucodoxF+honUF0y9rYmUhHwct7sgqDbGEGpWkdEEQRuwXt5frx0x10oVEzr1slt6YZbOnCqTUKBPG7SkjcLCdcohZQFgQe2j0VwbCaHk7N5S3xAMmlYBG5VvLO5cotbSLInrW8kxzPt3UPnl0rYNhoxJ2wtFCqGX9E5WNzLAfaykvlHdhmCwLZWeXNIrWks5hwslq15J40MGWkVTBmZNwBaCndy5RrihkXg3TXUahGoGSlmIN3DTfa9Y7NoCOCGA/pafT7Cm/enUKk3EAr4zHbi7fzS7WhXQyCJhfzOWUMuriHZBC4a9OOGA2l8/9icpwKqlaD2uorapqktmFlDnTdPM7032MwaKlTqnoOzxarVMnYq4Ko3BOZy3gRBqVpHudbAqKF4mW0vPGjo8nPt8yXaeQ5Mi0BJHwXWbiYBCwIP7BiL4PFTC2gIOAqCLSMaChX36uKXMzlMp2KOv9sz2TlzSAiBC0slbB0NY9TQtpeKVZRrDTRE63QywHmA/e999Vn8wt8+gv92zwv46pNnLa0qJF954gw+cd8xy3PSItiTjiOhBVqKyqS5LLWmhBaA30ctN9tyqYZyrWHWXrxpXwpA070i36PVIgh2lb7nbBHUWywCLzfZA0czGI0EcdXOZnmLtA6mJqJmPUk7v3Q7ZBaV01AaiT632LrZS6EKtNYRmDn1IT9u2J9GrlzDE6/0J41UCh3NaLGuHtMlM2vIg0VQtVsE3TWes7uG0gkNs7myJdg6n6+gIfSMtU61DfLalZXBTqnNbkgBnYq3CoJZN4vA7CLcTB+V32stYEHggR3jegop0Bxqr9IsKms9yflyDafniziwpfXvAL1/f8BHbQXBcqmGQqWObaNh0/++kK+4DqUBmgPs5QVWqzfw+UdfwXPnlvGp75/E+/73D00obKMAACAASURBVPF7//Rsy9/945Nn8XcPn7I8t6h0YdydipktIdT1AU1BQEQYiwRbXEOy/4qsvXjj3hR81EyjBay+YdkzSLqGvPQaajREc4auahFUG5YYAdAUkoVKDb/zpadaWjUIIfDA0QzetC9lGSH55n1pEMEMFEvsfulPf/8E/us/P9+yxidfWcAdnz2MOz57GB8zZlm0tQi0VotA9rsHWmMERaXK9vq9KQR85Jg99MjxOXz7+YuunwsAxzM5/NUDL7tau0XDJaO3Tg441xGUOve8anENae6umO+9NNsyA6RktwjiGmoNYbkGpZDek44j51LnYq692Lzmge5jBLGQ39TuJZNGMzynYyGPW4tFwDGCwUG2o46F/OYELpVmm4nWOMFLRrGYPdNIEvT7MJWMts0ckhuUbhFI11BVmUXQKgj09TYraGVrh/ffchmeu/NtuHZ6wjGbJJMtI5O1alKqdjSdirVYBPJmVYPoo9FgSwdSqcHKTS8Z1/Clf/t6/NobLzVfo1aUSiEmBYGXgebqpmivI5CuIRmIk+//5CuL+NLjZ/Dlx9WOKMDz55eRyZbx5v3WavbxWAjvfes+vOuaXZbn0/Fmq2EhBP76uydw91Ot7RXufuoc7n1xBq/MF1Cs1PGmfanOFoFNM5SbWijga7EIiopFENcC2JOOO9Z//K/7X8adX28VVCpfeeIs/tu/vIhjLkWParJCTLO2hZDnv9YQHXteNSu/9feSSoWTRfCVJ87gLx94uWUdqmW8dVS/Z88tNu9JKaRll9t2VoHZZ6gla6izMjKTLZsFdipTySgq9QZednADs0WwAZDtqC/bNmK6AlSkReDkFjhywTnlVKVT5pCsKt42GkbAr09wWipWlRu+NVgMWAfYy+E506kYgn4fdo5FXPOs7ZrUYrFq5t/vTsVwdqFoaV0gzWVpPgO60LCnj86YvtNmhsxrpybMXG3AWlGaVSyNuJFO16kuQnVNWCuLm66hqK1aVwq2B2z9Z+TjG/e3tjV53037cdPBLZbnVL/0yxk9bXM2V2nJBc9ky7hkIopvvO/N+NZv3YDPvfs6BPzut2JMa80akp+zazzS1iIA3AOvy6Uqzi4W22rG8nPsx0b9LOmSUdsiyDGPpmbfIU7QbBNuZA21ccVkcmUUKnXLNagnTTQFgXQ3qkqL/C77t+iCoF2HXCkI5LXplNrsRiZbchTs0hV6/5HW4L08bjLbr5dZf15gQeCBHWP6RXWZy2a+pY1FcORCDuGgz9GSkOyZjOPkXN41nbFpEegCaTwawkJBcQ0F3S0CuYHIAK+MVaQdAtzVesMsflPdXIuFCkYjetbTdCqKhgBOzzcLy5obdnNDH4u0diA1g2gj7tqv7gLS3y+nvK/pM+5gmquuCWuvISVryNa/R/ZPeuKVRYuWeP+RDA5uG8HkiHNqpx3dL61v/NIVU2+Ilr42M20qiZ1w6o0kj+UlE9FWi8AM4OqbiVu7hlxJr5S1pwNbPsfQot0K00rVpiautkUwxzxOeJv3K2MEYdMicD/f8rurtQD6OpqKiGwKeUoRBNJa2zup38ftAsZLpmtIjxE4pTa74VYguHM8ir2TccdjmTOuRdmGupdZf17wJAiI6Ke9PLdZmUpFEdf0algn4loAkaDfsd/Q0YtZ7N+ScLQkJHvScVTrAqddbsjzSyUQNQOK49Gg4RpyjxHI5+WNeXIuj0Q4YFY3pxMaKrWGpcGbTGUE7IKgasYmZLqk2nPIzBpSLIKxaKjlRpvJlvSZtA71FJKEFjDnEzezkQKeTXNXi6CqZA21WAQFBP2EekPgB8dmAeja8uOnFsxsIS+k4xrqhjWlatB2y2s2W0a6jTC0E3NIH5XxlksmdHeDqkQUlAAu0JqSK5HPuTUSVNf+6Il5x8wl1TWktkWQGUNe5/02s4aadQSA8wyKZi2ArR17sLmdRUMBbBnRWmZKx7UAto/pgr2dILC7hgD34+i0PtXqVblxfxqPnphvEezyuJkWQQ+z/rzg1SL4gMfnNiUj4SAe+92b8M5Xb3P8PRFhy4iGi06uoYtZ10CxRKaQujWfu7BUQjquIWi4D0ajISwVKmbDL6esIUC/mOTNe2I2j0tTzTTNZiWs4kNV1q/WRej51PoNIS0KdfPIlmrwUVPTBowOpA6a8OSIc3M1iaoJWlxDDp1JnZBavo+s2pRscQw4WwRv2pdGIhwwN/AfHJtFvSEc3UJuSMvh1HwBjxyfx1VGuq1dELTrLeRE1CF9NJMrw+8jbDfiV6p7qFStIxz0mcqHmyYrrQS31uJyrTvGIqjUG3j4eGv3l6KSrRPTAmZbBHnupSXc0SKwBYtHXGIE1XrDtLDUjbxoqyMAdKVFvU6lpi7rXNpN0VPdoRIvMwlK1TqWSzXX4P+NByZRqTfw0MvWYykFvfwOTll//aStICCiW4jo4wB2ENGfK/8+DWDtOiINAJFQ6xQwlclEa1HZfL6CTLbcNj4AAJd2aD53frmEbaNNDcNuEdizEyQxZYD98Uweu5UUVqcCqEyuuX67RSBN5LFoCGPRoKWoTOb6q8dnPBpEvlK35K/rNQTt3SxqKb/M2Za9huRntUNqUMm41hIsDtvrCCr6fORT8wXsnYzjjXtTeOBoBkLorp2EFsDVU+NtP09FHtO7f3gOlXoDP/3anQCsgqBYqSNbdt8onIhpAVTrwnIs9X73IdO6UQVBsVI3vyOgb6r2XH51ipubRdBoCMzmyrjliq0IB32OFcpqIVcs5DfbIki3za4JwyLwGiNQuo8CrQJkPl+B9Gaqikax2jqgaXcy1mIRpOOa6ffvFCOQ7lCJl2rndtPmAOCa6XFEQ37cbyvyK5T1eiApvIkIsdDazS3uZBGcA3AYQAnA48q/uwG8rb9L21hMjrQWixy5kAUA7O9gEYxGgkgnNFdBcGFJryqWyEBswaZF2JGVnqVqHeeWipZaBqcAt9vPas8VQLcK1M1j2ZhOZvlOst9QUXU3OQfRVKRLIFeuWbKRZPpeJ41MalDpuGaxCNQWE6GAT++LU6njwnIJlVoDU8kobtifxvmlEo5ezOGBoxlcvzdlWmFekDf/V394FpGgH++4Urcg1ZRSt2KjdjT9xc1NQWq3Mj5UqlhdQ+o1Edf0lhpqUFh1OZxwaSS4UKig1hDYOR7B6y9N4n6HgLHFNWT2t6qb2ra0CDpZcuWq1Z0VCfrh91GLS0m9LuVn1Bu6kLTfB1OpKGZzZXMzlT2dEloAPmqfNbRUbFrBEi/Vzu3mT+vfz4837Enh/iMZS3wubxPeAIwBSgMgCIQQTwkhPgNgrxDiM8bPd0MfQdnfRucbDCeL4OhFXRC4BZlV9qRjjmllQLOqWDIaCWK5VDNvEvcYgV6d+cp8AULAIgicmqTJn3eORyxttVXXEABMJ621BGrnUYmMKagBPdlwrh1SoCyXqpZCNTOdsJNryLhx0gnNoemc1YdcKNdMgTadjJlpon/93eM4v1TqKj4gPxPQtck37EliPBZCLOS3HmPD6urKInBoRZ3J6dqtdAsWqs3vqgZwATgeO7mhEVkDqirNTS2MGw9M4tRcocV6KNosAkAXWNJts3PCa4zAmjXUDM46JxwAzWurOabSup1NJ61uTCk8fT4y5mq3jxGoyg8AxD00wvNSKX7jgTTOLBQt93uhUjOnCkqcYkP9wqu68y0iGiGiCQBPQJ8k9tE+rmvDsWVEn5OqmnJHLmYxFg16uukvTcdxbCbXkieva8Y1m0WgX6AyrdSuSUjkAPvjhqWhCoKRcABawNciCEbCAV0QLDf72JSqDdM1JN/n/FLJvCnUzqMS6YeVQ3lK1TqypVrHDBx108qWavD7CJGgv206oYoMusn5ABK1jgAwrKVK3XQdTKVi2D4Wwf4tcfzDE2cANCuIvRLTAuZmeKMhROxtBbxsFHaiDq2o5aYmN2E1c6hQqVktgrC0ptRmfvrP06kYziw4p5CqWV7yWNjTSEvVOsIhq8tNtwj0876z6xiBzSdvc42oCor8jObgeut9IAv+Ts0VzOtPHnd9nKr7pr6guEPV9Xh1DbVTeOS1oaaR5sutFoGa9ddvvAqCUSHEMoCfBPBZIcR1AN7av2VtPJqTypoX6pELesZQu9iCZNd4VK8NsOWEy9RRS4zAyPw5t6hnE4WDzqdRDrB/4bxumagxAiJq3aQM03kyEcZFOcmp2BzgLblmegIA8AMj4JUt1SwZQwAwbQTAnzm7BKC1mMwNS4zAmHpGRIiHAiDq3OtFalCTCQ2lqt7at1bX/7dYBEYg/eRcHqGAD9sMAXXD/jSE0HPNZSC2G+T3u2H/pPnYKSC/Goug3hCYzen97qXmrwoCe+A07pDLL4X4lTtGXVNIVX/37lQMU8loiyAoVOqme6pZBKVbBJrRcTcS9HeMEZRsriG5brsAkWuKawFTo5dusRbXULJZS2A/7k7JDCpLBQfXkBbo6OKayZZB1NpyXsUpjbRQqbW0k1ez/vqNV0EQIKJtAH4GwNf7uJ4NyxbD5y4H1AghcPRC54whiUzrVFM4AaWGQNGkZXXxuaWiWd7vhDQ1nzu3hFRca9Ha7Y25pJa5ZUTDzLJeYyALy8YV7ei1U+NIaAHzQnZyDe0Yi2BPOoYHX9LTMb301NHX3EwTzRpBaEBv9hYPdc7aKFRqCPjIXG+hUmtxOwDNQPrJ2bylZ5DcwG88MNn2c9zYOhrGpekYLlEml1mysbJl+AhIxrqPEeTN1Ey93/1kIty0CNRgcbVhcQ05jgA13uvKHaMAnAPG9s3zxv1p/ODlWXPTFkIYQVqbRVCpY1HZSJ186x/4yjP4zovN9hb2rKHm37W6hkbCAUyOaKZGr46pVIlpAUwmNJyczZsKjykIOrmGiq2uoUS4mdrsRiZbRjIWalsgCLSmkTrFCNSsv1K1jrd99EHc88z5tu+7UrwKgjuhTxN7WQjxGBFdCuClDn8zVJgWgbHhnV8qIVuudcwYkkgNwt6fRxapbRmxBosB4Nxi0TU+ADRvzGfPLmM65dAaw1EQhDGZCKNs1Bg45VMH/T5cvzeF+4/MtMwiUHnz/jQeOT6HUrWumMydXEPNZmN2ARMPBzrHCMp68NJs41upO7odZEfPU3MFS8+g6y6dwLvfOI2fv26q7ee4cedtV+ATP3e1+djeDjmTLSMZ1yy9izohLblnDetK3aBNi8CSNWR3DbnHCK6QgsAhTjCTLVuO5VW7xlCqNnDOaE1SrjUgBJT00aYLS/exh8zPVy25Sk3ve3XvC03XSLlWRyjgsyg1k4lwy+Q/abWORYLKTA7DNeSQNLE7GcOpuUJLNo/uGnK2CJaMjDx74aOZyNDmGtSzuToLeZlGKlNyC2Vni0AmPzxyYh5HLmbb3u+rwZMgEEJ8SQjxaiHEvzMeHxdC/FRfVrRBkb5v6QI5YgSKvQoC6e6Zs3UEnTOmm6nzjqUgmMmWXWsIgOaNeWG55Nj9VHcNWd0W6bhm3gCZbMk0n0dtZvKPXqZn2By5mEXWIWsI0AVBudbAoyfmm+0lOhRShQI+aMZ8YnvswUvWhm5iB5odRss1pY+NNVicLemuIVVIBv0+/N47D5oafbfs35KwdKhNJzQsl2qmFt1tDQGgKwGXbU2YFphFEARbK1DtLZnluVFdQ3Iz252MIa4FXC0C67hFa6aZdEeZ/XFUi6Cojnm0FmLJa1r10esFf9btaCoZbZmIZ9YCKBu5FIJhJ0GQiuLEXN4MfEuLtF2wWArF3bamgl46orabNqdyzfQ4IkG/eU4LThZBqNk08v4jM9ACPtei1tXitbJ4JxH9IxHNGP/+gYh29mVFG5SEFkA46MPpBT34aKaOTnbnGrK3hp7LtU43kzeYEEDUYSiNRL2wplPxlt9PJsJYKFRRqTWQL9eQr9SRTmjNGoPlcks7Xol0oXzz2Yuo1oWjRfC66aQ5pH4mW0LAR5iIuvtOJTJImCtbYw9xLdCxZ03eSGds1grUmy2OLa4hP07NFVCuNVq6iPYSuXnOGhtRJlfuKAyduOFAGodPzSNXrlm0W6kIlCwWQcMM4ALOmmwzNTeA3amo4yxqu9Cy157Yexqpaa5LStZNwjZdzqlFhD2YD+gbca0hrI3jDKtVde2UXFxDgB4wzmTLODWbt/jux6K6cHJq62IKApvy5CVhYbZNVbGKnkaaxP1Hdas6X6lZCjIBI33UsAgeOJLB6y5NOgq7XuDVNfQp6Gmj241/XzOeYwz0PjxxfPahU/ixjzyAf3j8DLaOhFs0aTekRTDfYhFUkLJNNxsJB0zXgheLAICja0je2LO5srlRyWAxoN/wiw7BYkD3hV++bQR3P3UWABwtgkjIj2t3T+CBoxnMLOsmc7tWGxKpQdpdQ4lw0INrqGbJ3tEtglbXUDQUMDcyu+bXS+yb58xy9xYBANy4fxLVut4CQ81VjzoFi22uIXkM1UBnrqxXg0dDfkwlWzvKAq1Cy55y3EzbbFYWA82soXFlupe6eTqNi1SbAkrkRmxvHCeLwtRxreo6LO9hnNvHTi1YfPdSSDkFf08ZQlH2K5J0mlsshHDtM+TEjQfSOD1fxInZPPLlmlmHIZEWwam5PI7P5s1so37gVRCkhRCfEkLUjH+fBtC/VW1Q/v7XrsMf3fYqjMdCOJbJmRO9vDASDiDgo1ZBkCtjIm7VoonIDBh7iREAzhaB3JBk62nAEARKvGOhUEHI73P0v954IG3mQrv1D3rz/hRemsnh6TNLnm8QOZ84V6pZLKG4F9eQESOIWmIEra4hVfva7SAke4W6ecpK3W4yhiSvnRpHXAvg/qMZZBTfvayWlpuhDOCq14VmFNCpLo1sqVkNPp10TiG1WwRjxoB6KYiKtoJGsy2CkTXU7OVvTbs0BYHdIrBlv+1OWjuIqlbrWCSErDFTwL4Oy3sY5/bZs0tIK5q6TA11yhw6OZvH9tFwi/ad6FDUuFSsolJveD6/0qr+1vO6Ve1kETQE8M3nLgBYeQKDF7wKgjki+nki8hv/fh5Aa+ORIWciFsIvvH43vvgbr8djv3sT/vRnrvL8t0SE8VjI0SJwyjAZM9vjuruG5CaqD1Fxn6NgEQRxzXRzzSyXdRM/GnTMTFL78Di5hgCYRVpHLmY9V9NKDVJOJ5OMOOSV29FN7KZFkFMsAvXGloIi5PdZivV6jSoIFotV1BpiRYIgFPDhDXuSeOBIRu9earyHz0fQAj7TPSKn1qnflYgscx4AWI7tVDLakkJartWxVKxa1urzkWXurt0ikG0RZnMVfcyjS4zA0TVUbXUNyRjISaPBoWq1yut/uVg1v7uTRSDdfnXbcTfbTDjUEpycyzu6C9vNSFC/l9fze0kyiktTMTMTyClGAAD//MwFTCWjrlMOe4FXQfCr0FNHLwA4D+BfA/jlPq1pU5CKa649gNxIOgmCXMUSKJZIs7u9RaD/bvtoxNG3qLotVHcDEemV0tlyS1WxytVT483xlA6uIQA4sCVhtun26huPawHM5Suo1BstrqHO6aN1RLWApcOoU466FBSXJKNdZfB0y0QsBCJ9k2im0Hpra23nxgOTOLtYxGMn5i2aut6UTv+Obv5y3cqyxgjksZ12ccEArZuamg5rDxYDuhYrs4rUrKFcuWbOZZDXmtToAWfXEBFhKhk1K5/VYq0xZSNv12olrgXMLB71mMn7Z8khYHxqruBoJXaKEXTqM+TEDQfSeOqMng3mlDUEAE+dXuy6uLFbukkf/SUhRFoIMQldMPxh/5Y1nIxHrYJACIG5vHM62pgH15AURG6aRMrmGvIpwbRJI6No0aHCUhL0+8xhG24WgTqkPu1xA0yEg+ZmkrAFi+09c+zky3rQTe0w2pyH22oR7F5hdpBXgn4fJqIhZHLlrjVGO7LlxYXlkuU9IkF/y9hK+6Zo99OrKb9TydaOsq6CQLEI5AasKhmxUKApCKLNYDEA5IwMGLVduyxYtLcAkexW4hfqmlTXTrusIaAZH1O/y5g56c+qeC2XqpjLVxzjRp0sAq+ZcSqqu8euOKpu0X7GBwDvguDVam8hIcQ8gNf0Z0nDy0Q8ZBliUqjo7R2cqhTljdAuWKwFfAj4yFUQBP0+TMRCmMmWWvLbZRM9e8M5O++4cjtCAZ+lzsGO1Ga6cQ1Jd47VIuicx12o1BHTApYOo44FZcZx62fGkERq0asVBDvGIuaYRYsgCDUFgVvgVM+4ao0RAEAqHkJcC5hBUkDVbq3nVa1GLzkInajmN0eg2sc8yvOmNuGTcQI9RuDk44/h9HwR9YawWK3yvRcL+qQ+LeBztezkObYIgmjz71VOGW4op+tCC/gR8vtcZ2ev5PxeNz2hDEyyN50z3JcBH15/acrze64Er4LAR0RmP16j51B3fg+mIxM2i0BWGScdBMF4tLNFQET4yM/+CH7tTdOur5Eanj0wOJkII7Pc3jUEAO949TYc/uBNbUvqbzyQxo8f3ILr93q7mC2bv6aMsdSspvlcrmz2UQJgScMLBXwI+X3IuwSLpaCwpwj2A7l5rqTzqB2zh1HcJgjkZDAXN4ndT69bBPqxJSI93161CHLO2u1kQsN8vox6QyiDkZrnSzY6BJp+eLtLJZMtm9eUnARWrra6hgDdYqvU9SK2TFafwTButEIHDEFga7Ln9B6AdYNOhIMgao0RnDBTR50txUSbosZMrtxx8JKdcNCP1+/RawNaKqONx9dNT7T9fr3AqyD4UwAPEdEfEdEfAfgBgD/p37KGk4lYCEvFqpnbLAtvHF1DHoLFAHDrVdvbar2TI5rutrBls6QTGrLlGmZzlZYaAjv21hV2EuEg7vrFQ56DXfZqYvV9gGZh1O/+47N492cOm78vVfVKV6lJRY1Zv07po1KQyqFA/SSd0DCbLbdU6q4E6UpocQ1VbK6hUKtrKGfrNaQe293JmKUNesalZ046oaEh9GvTyQ2lujPGzPRRWYhVNVMs99vGRVZcXENq4zjZvsHvI8twmaLS78gJKexVAez3EUbCQSzZXEOnDGE4NeF8XcTD7v2GZOqol95iKjLpwt6vSz7ud3wA8KjVCyE+S0SHAbzFeOonhRDP929Zw8lELAQh9Is7FdeaFoFDsNh0Da2ywCQd13A8k0dDCMvcBHnT1BvCcy1Er4grVoCTayhbqqFSa+DBlzKo1hsQQoCIzCpMqUnpedj1ll73AHDN7gl8+leuwev7VKmpIl1DarbPSnndpUn8/k8cxC1XNKflRUIB09fubhFYYwT2Go3XTo3j60+fxytzBVySjGImW8ZENNQyj0HNgpKuoXBItbRahW1z7KSe/lms1rF3SxyPnpy3uIbcKoOBZuM4+fmJsN6EcKlQQUHpgOrETZdvwe//xEEcsg0ZGosGWyyCk3MFbB0Ju2rg20bDZgzEzky2tKLz+7PXXIJQwI/Lt45Ynj+wJYH/5/+4Ev/qNdu7fs9u8ayaGBs/b/59ZEKpLk7FNdMicHK7eMka8kLasAiEsKbXqe2ipfa1Vri5hlRf8+GT86YLYqmoB7RlFaa0kuSIR6cYgc9Hfc3LVknHNVTqDbw8k1tRMZmK30f4leutrr5I0IeLS+0tApk1JIRAtS5QrjUsLgyzzfRLGfxCcsq1MEoVBIWK3iY85FdjL820XCmMRpTzJt1jMtaxaAaLnV1DWxJhaAEfThltItS02dGIvpGXKq1jKlXCQX/LMQOcG8/pqaPuCQS7kzF8+4WLjr/LZMsrSvGMhPz4uesuaXmeiByf7wfexy8xfWfC1m9o1owRuLuGVus7TMf1IfbVurBsUlsU3/D4WlsEDlaA/nPTNaROy5LBS5nNIdPwoloAuXKz6Vyoi2ljvURuXi/NZFfUXqITapV0uxhBrSFQqjbM46Sm/E6nYtg1ETHHUboKgniz31Cx0mjpfitnJ4wqtSdqjEAKgkvTcVOjB5zrCAB9w59KRnFituBY4CZjBCtRiJxmEpyay7fdzKeSMczmKo5pzN1UFQ8aLAgGiAlbv6H5fAWxkN9xs59OxRAK+HCpQ8VwN9jjAhI1132tXUMjLjECtWfO/UdmTI22mc4oJ7bpz8dCMkZQR8BHHVsD9wt5XO3CtleEg806gmZLZmd/c7ZcNTcx1Z9PRHjzvjQeenkWlVrDtTmeaRHk9BiB3Z0jLQI100yNEchaiq0jYVOjB5wriyW7kzGcmM21VGWPGht5odK6Di+MRa0xgmypitlcpW1MTQaeT9l6M1VqDSwUqi1ZVhsFFgQDhN0icGovIdk+FsGLd96MK3eOruoz1Q1fvcnGo3o7Af3ntXUNyRhBOOiz+KildXDkYhZHL+bwzqt036ncXOQQD9MiMGMEzoHItWLSRdj2ikjQb/rrXS0CJeNKxgrstR837E8jX6nj8Kl53Q3jYL1EQn4ktABmlssoVmotmri0CNRMs2jQrw8VUiyCdEIzZm9X0WgIVOru52h3Kobjs3ldkNpqAZYKFX0050oEQSSIBcU1JDf3drUlTv2PgGZiB1sEzKqRG660CNzaS0i8NHDrhJtFQERK7/a1tQjcqpVlz5x/eUbvvfLTh/QGuKZFULZZBFozRuCUo75WqIV0K60qboeMhcg+Q4A1gAu0jgAFWrNU3rA3hYCP8PWnz6NSa7haL+mEZloE9g1YWgSjSlzJ5yNz2lgmW0bARxiLBE2LoFJvzepSmUpGISe42msBpEWwEtfQaDSE5VIVdaPi2a3rqH0tQKtF4HUC36DCgmCACBk5yGqMIOViEfQKp+6S5mMjYLxewWJ7PrbsmTOXr2DHWASv2TWGcLA5d1laBNLlEQ3pU8hK1TrC62gRjIQDCBmf3xeLIKQ3J6sYDdjsAVxAcasZ7b31dQVbXvPaqXF87alzbdeaMrKgitVGS7aO3JCdxjxKQSC70ErXTLPy2901JHGLEawkVjYWCUKIZhM5WVndLlgcDelTz07Y5jf0okZkGZl7EQAAE/JJREFUPWFBMGBMxENm2ft8vtzVSMOVkND0IfZOhTCTCQ1awNf3YhY7MWM+sVPbCrmh3XAg3TJ3uRkj8BuvHQyLQLWu+iEIpH+8VGmgYGTQ2HPZzUB7yTlGILnhQNq0GNzWKusiipVaS/6+rJFoHfMYRK5ctWT+jMmsH1nw5xIjUDdmq0Wga/T5cm3FMQKgWctwcq6AyYTWsTZHn3pmEwQ5tghcIaKbiegIER0jove3ed1PEZEgokP9XM9GQPYbEkJgLldxjRH0CrmZOhXCXLF91FJbsFbI+cROjeykcLjRbFsRbloEZRkjaFoEBSNvfT1jBEBzg+iHxmgOhKnWXLVjtQajmTXkIAiU4iW3tU6aFkHrZ7lZBLKFeCZbNt93zIgROPWCUtk2GjEtHLtrSAis2DU0ZutAemou76nS3GmQj7wGnWp+NgJ9uzuIyA/gEwBuAXAQwO1EdNDhdQkA7wXwSL/WspGQHUiXizXUGsKxvUSv2ToSduwV9B/euhd3v+f6vn++E/FwwFFjjWsBBP2ENxjtKtJxq0Xgo6aLQQaNFwuVgRAETpW6vcAcYF+puwZOVUHgFiMAgIPbRszN1i0DRladL+SrrTECaRHYEgwSRgdStahuNBLEcqmKQlVfj9s58vsIlySjiAT9tsrlprBZSbBYxjFOGcVqJ2YLnpoQyqlneaV300y2hLFo0FWYDTr97Bd0LYBjQojjAEBEXwBwG1qL0v4IwB8D+J0+rmXDMB4L4fnzy5ht016i1/zBra8yg3Eq3ZbK95J3v3HaMY3vDXtS2DMZNzeEyRENDxkDwPPluuFW0tctTfz5DkH3teDSdAxTF6N9SWGVWnmhUkfBNp1MElNiBIVK3ZgN3fo6IsIN+9O455nzGIk4bw/SzXV+qWj2yZHImhP7dRvXAjieyWNOdQ0ZGr3UptsJ6+lUDLV6w3JNqrGrSAd3Trvv8d4v/FD5nM7p2DJmcXIuj1dt17P2zi+W+pIavFb0UxDsAHBaeXwGwHXqC4joagC7hBD/TESugoCI7gBwBwBccsnaVNqtF9IiaNdeotdcsWN1Kaj94NfedKnj8++9aZ/lcTquYalYRbmmb4JRpae7tAgWClVsH+vf8Bkv/Meb9uMOl++0WuTGX6rWUaw2HF1DQaPSN1vSs2zaNUb7zzdfhp+9ZperIiA38oZo1cT3TibwmV+9FtfbBIRsLd4QsAgCALhoZNy0i+N84JbLWoq/RldpEVySjOIv/s3VmM1XACHg9/nwjiu3dfw7NXPoVdtHUa038OiJebzdw98OKuvWQZSIfAA+Ag8DboQQdwG4CwAOHTrkoLtuHsZjIZRrDZye132Q/XAlbCbUKWu5cs3SyldaBIPgGgoH/X0bPC43/mK13rbdghwQU6jUXedHADBjRu1+b/9sFacmaYlwADUjTdNMSzY0+ovLeh1Iu3N0abpVU1cD0pHQys7vLSvYvO21BE++sohsudb3mQH9pJ93x1kAu5THO43nJAkAVwC4n4hOAngdgLuHPWAsN/6XZvROkGvhGtrIWHvf1K0WgSEIGsI9ELkZkBt/oVJHodpa5CVJGJ0zs6WaY3zAK/bOp15QLRAzRmBo9DMeBIETahwiElw7nVZOPZOzC+4/MgO/j3D9vv7ODOgn/RQEjwHYR0TTRBQC8C4Ad8tfCiGWhBApIcRuIcRuAA8DuFUIcdj57YaDCePiPjaTBbD2Vb0bDVmgNWME7yy98RWhsN4WQT+RWnmpWkex4t6JM2E0nsuVapZmft2SjGmQtYxeU4tVwSPPmby2TddQl8JabUWy1inOu5NR0yJ44GgGr71kvGM79kGmb3eHEKIG4D0AvgngBQBfFEI8R0R3EtGt/frcjY5MFz16MWcpRGKcsVsEalaJ6iZyy1HfDEgLoFjRBYGbli5nPi/bZhF0i99HSBqWqmeLQNkkUwn9GpeunYtGi5Buz1HA7zNdXKttx94tu1P6CM2ZbAnPnVs2x4huVPpqTwkh7gFwj+25D7m89sZ+rmWjIC2C0wsFTK/BGMWNTlIZDp+v1DAVaqb/qS6SYXENtevEGdcCmMmWOsYIvCAn23m2CAwBHVfGiI5EpGuoc9aQG2NRffLaatuxd8vuZBRffryMbz6rtzvZyPEBgCuLBw5pEQixcYtT1pKA34dkLISZbBkFI31UoloHm9k1JIPQxWrdsf+PRA6nyZZqXY1TdEJaYl41cenGUeML+pSwgNk0cCXCWgac+xWId0OmNn/u4VNIJzQc3DbS4S8Gm817d2xQEkbBFOA8h4BpJWVop3lb+qglRrCOLSb6jRbwwUdAvlxDySV9FGhW96rzildKt4JAuqLsufZj0RCqdT2baCXuu17N5egWWUtw9GION+xPr2vNTS9gQTBgEJEZROt3e4nNwuRIGBnD5aFaBCG/DwEjqrmZLQIiQiToN1squ1oEmp4+Wm+IVcUIgGb7Ca8uGSl47GmpanXwylxDxqS+tbYIlOH2G90tBLAgGEhkCmmKawg8kY5rOLNQRL0hLFYAEZkb1WYWBICuEcv25W7asWoFrDpGYGzo7WYFq0g3nV0QjBpxAqKVTZCTAee1tghGwkEkYyH4CHjT3o0vCNatoIxxRwqCJNcQeGJyRDNbd8dsrQZimp47v5ldQ4C+Ec5LQdCmoMz8eZUxgl3jukY84TG9eSQSQCzkbxkDKTV6LeBbkXtl21jY7KC71ly2LYF6Q6z5BL9+wIJgABk3BQFbBF5Q/c4tE7OGxSII+s0pWe4WQfN2X23O+1sum8TXf/ONnrp1Anog+N7fvrGlUl5q9CvN6vqVN0zjHVduWxcf/cdvvxobOzLQhAXBACI7jnJ7CW+ow3ViWqtFAAyBIAgF8IpR4NQufdT8eZWuIZ+Puu5RtXW0tZupjBGs9PxEQv62M4b7yWa6Pzf33bFBkcFibi/hDdUisAuCpkWwyV1DQZ/ZlM0tlbKXMYJeYbqGNnHB30aAj/4AMp2KIRz0Oc4IYFpRA5AxmzYsYwabfaOJBP1mK/F2dQSS1cYIesVqXUNMbxiMq4GxcOtV2/GGvUkzo4Jpz6QiMO1jBofFNRR16LpqRxUEq60j6BWrdQ0xvYGP/gDi85HZmIvpTCzkN7XgmGazCLThcA2p7iDXrCFtAC0CFgQDAR99ZsNDRGbA2K4Ny8ebfaNRA8Rhl978+vQ2XXD6fYOR7yLHRW52QT3obO67gxkaZMC4xSIwNsjwZo8RKILAzTXk8xHiocCqM4Z6iWkRbPLzM+gMzhXBMKtgckQfDh+2aZZRM0awuTVOL64hQI8TRAfELQQ0K4s3u8U26AzOFcEwq2D7aARjkSB8NpdHKq7B76OBSZfsF9I1FAr42rp94uGAq8WwHgT9PsS1wKYX1IPO4FwRDLMK/s8f3YvbfmRHy/O3XrUdB7eNWMYabkakFdCpG+glE7E1793fiT2Tcewcj6z3MoYaFgTMpmAiFnKs9AwFfDi4fWP3iveCjBF0EgQfv/01GLSOyV/8jdfBP2iLGjJYEDDMJkAKgE7a/lp36fQCu4XWH47QMMwmQAqCtZ7UxWwOWBAwzCZAWgKD5v9nNgYsCBhmEyAHxAyi64cZfFgQMMwmgF1DzGpgQcAwmwB2DTGrgQUBw2wCvNYRMIwTLAgYZhMQ4RgBswpYEDDMJiDMFgGzCrigjGE2AUG/D+/50b348VdtWe+lMBsQFgQMs0n4v952YL2XwGxQ2DXEMAwz5LAgYBiGGXJYEDAMwww5LAgYhmGGHBYEDMMwQw4LAoZhmCGnr4KAiG4moiNEdIyI3u/w+98ioueJ6GkiupeIpvq5HoZhGKaVvgkCIvID+ASAWwAcBHA7ER20vexJAIeEEK8G8GUAf9Kv9TAMwzDO9NMiuBbAMSHEcSFEBcAXANymvkAIcZ8QomA8fBjAzj6uh2EYhnGgn4JgB4DTyuMzxnNuvBvAvzj9gojuIKLDRHQ4k8n0cIkMwzDMQASLiejnARwC8GGn3wsh7hJCHBJCHEqn02u7OIZhmE1OP3sNnQWwS3m803jOAhHdBOB3AdwghCj3cT0MwzCMA/20CB4DsI+IpokoBOBdAO5WX0BErwHwVwBuFULM9HEtDMMwjAt9EwRCiBqA9wD4JoAXAHxRCPEcEd1JRLcaL/swgDiALxHRD4nobpe3YxiGYfpEX9tQCyHuAXCP7bkPKT/f1M/PZxiGYTozEMFihmEYZv1gQcAwDDPksCBgGIYZclgQMAzDDDksCBiGYYYcFgQMwzBDDgsChmGYIYcFAcMwzJDDgoBhGGbIYUHAMAwz5LAgYBiGGXJYEDAMwww5LAgYhmGGHBYEDMMwQw4LAoZhmCGHBQHDMMyQw4KAYRhmyGFBwDAMM+SwIGAYhhlyWBAwDMMMOSwIGIZhhhwWBAzDMEMOCwKGYZghhwUBwzDMkMOCgGEYZshhQcAwDDPksCBgGIYZclgQMAzDDDksCBiGYYYcFgQMwzBDDgsChmGYIYcFAcMwzJDDgoBhGGbIYUHAMAwz5LAgYBiGGXL6KgiI6GYiOkJEx4jo/Q6/14jofxu/f4SIdvdzPQzDMEwrfRMEROQH8AkAtwA4COB2Ijpoe9m7ASwIIfYC+CiAP+7XehiGYRhn+mkRXAvgmBDiuBCiAuALAG6zveY2AJ8xfv4ygLcSEfVxTQzDMIyNQB/feweA08rjMwCuc3uNEKJGREsAkgBm1RcR0R0A7jAe5ojoyArXlLK/95AwjN97GL8zMJzfexi/M9D9955y+0U/BUHPEELcBeCu1b4PER0WQhzqwZI2FMP4vYfxOwPD+b2H8TsDvf3e/XQNnQWwS3m803jO8TVEFAAwCmCuj2tiGIZhbPRTEDwGYB8RTRNRCMC7ANxte83dAH7J+PlfA/iOEEL0cU0MwzCMjb65hgyf/3sAfBOAH8AnhRDPEdGdAA4LIe4G8LcAPkdExwDMQxcW/WTV7qUNyjB+72H8zsBwfu9h/M5AD783sQLOMAwz3HBlMcMwzJDDgoBhGGbIGRpB0KndxWaAiHYR0X1E9DwRPUdE7zWenyCibxHRS8b/4+u91l5DRH4iepKIvm48njbalhwz2piE1nuNvYaIxojoy0T0IhG9QESvH5Jz/R+N6/tZIvo8EYU32/kmok8S0QwRPas853huSefPje/+NBFd3e3nDYUg8NjuYjNQA/DbQoiDAF4H4N8b3/P9AO4VQuwDcK/xeLPxXgAvKI//GMBHjfYlC9DbmWw2PgbgG0KIywBcBf37b+pzTUQ7APwHAIeEEFdAT0R5Fzbf+f40gJttz7md21sA7DP+3QHgL7r9sKEQBPDW7mLDI4Q4L4R4wvg5C31j2AFrK4/PAPhX67PC/kBEOwG8A8DfGI8JwFugty0BNud3HgXwZuiZdxBCVIQQi9jk59ogACBi1B5FAZzHJjvfQogHoWdSqrid29sAfFboPAxgjIi2dfN5wyIInNpd7FintawJRifX1wB4BMAWIcR541cXAGxZp2X1iz8D8J8ANIzHSQCLQoia8Xgznu9pABkAnzJcYn9DRDFs8nMthDgL4H8AeAW6AFgC8Dg2//kG3M/tqve3YREEQwURxQH8A4D3CSGW1d8ZBXubJmeYiN4JYEYI8fh6r2WNCQC4GsBfCCFeAyAPmxtos51rADD84rdBF4TbAcTQ6kLZ9PT63A6LIPDS7mJTQERB6ELg74UQXzGevihNReP/mfVaXx+4HsCtRHQSusvvLdB952OG6wDYnOf7DIAzQohHjMdfhi4YNvO5BoCbAJwQQmSEEFUAX4F+DWz28w24n9tV72/DIgi8tLvY8Bi+8b8F8IIQ4iPKr9RWHr8E4J/Wem39QgjxASHETiHEbujn9TtCiH8D4D7obUuATfadAUAIcQHAaSI6YDz1VgDPYxOfa4NXALyOiKLG9S6/96Y+3wZu5/ZuAL9oZA+9DsCS4kLyhhBiKP4BeDuAowBeBvC7672ePn3HN0I3F58G8EPj39uh+8zvBfASgG8DmFjvtfbp+98I4OvGz5cCeBTAMQBfAqCt9/r68H1/BMBh43x/FcD4MJxrAH8I4EUAzwL4HABts51vAJ+HHgOpQrf+3u12bgEQ9KzIlwE8Az2jqqvP4xYTDMMwQ86wuIYYhmEYF1gQMAzDDDksCBiGYYYcFgQMwzBDDgsChmGYIYcFAcN0ARG9j4ii670OhuklnD7KMF1gVDAfEkLMrvdaGKZXsEXAMC4QUYyI/pmInjJ63/8+9P429xHRfcZrfpyIHiKiJ4joS0afJxDRSSL6EyJ6hogeJaK9xvM/bbzXU0T04Pp9O4ZpwoKAYdy5GcA5IcRVQu99/2cAzgH4USHEjxJRCsAHAdwkhLgaepXvbyl/vySEuBLA/zT+FgA+BOBtQoirANy6Vl+EYdrBgoBh3HkGwI8R0R8T0ZuEEEu2378O+qCj7xPRD6H3f5lSfv955f/XGz9/H8CniejXoQ9VYZh1J9D5JQwznAghjhpj/94O4L8Q0b22lxCAbwkhbnd7C/vPQoh/S0TXQR+k8zgRvVYIMdfrtTNMN7BFwDAuENF2AAUhxN8B+DD0Ns9ZAAnjJQ8DuF7x/8eIaL/yFj+r/P+Q8Zo9QohHhBAfgj5YRm0fzDDrAlsEDOPOlQA+TEQN6F0g/x10F883iOicESf4ZQCfJyLN+JsPQu9yCwDjRPQ0gDIAaTV8mIj2Qbcm7gXw1Np8FYZxh9NHGaYPcJops5Fg1xDDMMyQwxYBwzDMkMMWAcMwzJDDgoBhGGbIYUHAMAwz5LAgYBiGGXJYEDAMwww5/z//ufFb2ddEwgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "n_layers = 2\n",
        "batch_size = 20\n",
        "steps = 100\n",
        "trained_params, trained_bias, loss_history = quantum_model_train(n_layers, steps, batch_size)\n",
        "\n",
        "pred_test = quantum_model_predict(X_test, trained_params, trained_bias)\n",
        "print(\"accuracy on test set:\", accuracy_score(pred_test, Y_test))\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.ylim((0, 1))\n",
        "plt.xlabel(\"steps\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owj-H5IXuGlk"
      },
      "source": [
        "How often was the device executed?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QBmGLesj_90",
        "outputId": "4cfaa8ca-5ffc-4028-baf7-1c2ca03618ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "76152"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_var.num_executions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lG8OWUOuNMM"
      },
      "source": [
        " In each optimization step, the variational circuit needs to compute the partial derivative of all trainable parameters for each sample in a batch. Using parameter-shift rules, we require roughly two circuit evaluations per partial derivative. Prediction uses only one circuit evaluation per sample.\n",
        "\n",
        "We can formulate this as another function that will be used in the scaling plot below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZadxCJHkCbs"
      },
      "outputs": [],
      "source": [
        "def circuit_evals_variational(n_data, n_params, n_steps, shift_terms, split, batch_size):\n",
        "    \"\"\"Compute how many circuit evaluations are needed for\n",
        "       variational training and prediction.\"\"\"\n",
        "\n",
        "    M = int(np.ceil(split * n_data))\n",
        "    Mpred = n_data - M\n",
        "\n",
        "    n_training = n_params * n_steps * batch_size * shift_terms\n",
        "    n_prediction = Mpred\n",
        "\n",
        "    return n_training + n_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCUleFZKuQXG"
      },
      "source": [
        "This estimates the circuit evaluations in variational training as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsLXhUn6kGnh",
        "outputId": "e51ef4fa-2eb5-4133-ae07-41865cf594d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "83287"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "circuit_evals_variational(\n",
        "    n_data=len(X),\n",
        "    n_params=len(trained_params.flatten()),\n",
        "    n_steps=steps,\n",
        "    shift_terms=2,\n",
        "    split=len(X_train) /(len(X_train) + len(X_test)),\n",
        "    batch_size=batch_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt8eb4LNuTfQ"
      },
      "source": [
        "It is important to note that while they are trained in a similar manner, the number of variational circuit evaluations differs from the number of neural network model evaluations in classical machine learning, which would be given by:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ5Fb60ZkIyJ"
      },
      "outputs": [],
      "source": [
        "def model_evals_nn(n_data, n_params, n_steps, split, batch_size):\n",
        "    \"\"\"Compute how many model evaluations are needed for neural\n",
        "       network training and prediction.\"\"\"\n",
        "\n",
        "    M = int(np.ceil(split * n_data))\n",
        "    Mpred = n_data - M\n",
        "\n",
        "    n_training = n_steps * batch_size\n",
        "    n_prediction = Mpred\n",
        "\n",
        "    return n_training + n_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1uSOBj1uYSl"
      },
      "source": [
        "In each step of neural network training, and due to the clever implementations of automatic differentiation, the backpropagation algorithm can compute a gradient for all parameters in (more-or-less) a single run. For all we know at this stage, the no-cloning principle prevents variational circuits from using these tricks, which leads to `n_training` in `circuit_evals_variational` depending on the number of parameters, but not in `model_evals_nn`.\n",
        "\n",
        "For the same example as used here, a neural network would therefore have far fewer model evaluations than both variational and kernel-based training:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lQsafV8kK6U",
        "outputId": "bb9a6d30-c3c8-4e7a-f1de-8b4dca8ba33f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13287"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_evals_nn(\n",
        "    n_data=len(X),\n",
        "    n_params=len(trained_params.flatten()),\n",
        "    n_steps=steps,\n",
        "    split=len(X_train) /(len(X_train) + len(X_test)),\n",
        "    batch_size=batch_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYst1Sc9ufte"
      },
      "source": [
        "## Which Method scales the best?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "0nbi2Qk-kNVI",
        "outputId": "e0595b74-d906-460a-919d-ae676d856dd1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "variational_training1 = []\n",
        "variational_training2 = []\n",
        "kernelbased_training = []\n",
        "nn_training = []\n",
        "x_axis = range(0, 2000, 100)\n",
        "\n",
        "for M in x_axis:\n",
        "\n",
        "    var1 = circuit_evals_variational(\n",
        "        n_data=M, n_params=M, n_steps=M,  shift_terms=2, split=0.75, batch_size=1\n",
        "    )\n",
        "    variational_training1.append(var1)\n",
        "\n",
        "    var2 = circuit_evals_variational(\n",
        "        n_data=M, n_params=round(np.sqrt(M)), n_steps=M,\n",
        "        shift_terms=2, split=0.75, batch_size=1\n",
        "    )\n",
        "    variational_training2.append(var2)\n",
        "\n",
        "    kernel = circuit_evals_kernel(n_data=M, split=0.75)\n",
        "    kernelbased_training.append(kernel)\n",
        "\n",
        "    nn = model_evals_nn(\n",
        "        n_data=M, n_params=M, n_steps=M, split=0.75, batch_size=1\n",
        "    )\n",
        "    nn_training.append(nn)\n",
        "\n",
        "\n",
        "plt.plot(x_axis, nn_training, linestyle='--', label=\"neural net\")\n",
        "plt.plot(x_axis, variational_training1, label=\"var. circuit (linear param scaling)\")\n",
        "plt.plot(x_axis, variational_training2, label=\"var. circuit (srqt param scaling)\")\n",
        "plt.plot(x_axis, kernelbased_training, label=\"(quantum) kernel\")\n",
        "plt.xlabel(\"size of data set\")\n",
        "plt.ylabel(\"number of evaluations\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-7N5ClJkSXL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "c0f2ef85a013b3e049ebe5ad4c39b42bd34e85c4b0686b7bef6ce5ff62aa91e1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
